{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [
        "vncDsAP0Gaoa",
        "FJNUwmbgGyua",
        "w6K7xa23Elo4",
        "yQaldy8SH6Dl",
        "mDgbUHAGgjLW",
        "O_i_v8NEhb9l",
        "HhfV-JJviCcP",
        "Y3lxredqlCYt",
        "3RnN4peoiCZX",
        "x71ZqKXriCWQ",
        "7hBIi_osiCS2",
        "JlHwYmJAmNHm",
        "35m5QtbWiB9F",
        "PoPl-ycgm1ru",
        "H0kj-8xxnORC",
        "nA9Y7ga8ng1Z",
        "PBTbrJXOngz2",
        "u3PMJOP6ngxN",
        "dauF4eBmngu3",
        "bKJF3rekwFvQ",
        "MSa1f5Uengrz",
        "GF8Ens_Soomf",
        "0wOQAZs5pc--",
        "K5QZ13OEpz2H",
        "lQ7QKXXCp7Bj",
        "448CDAPjqfQr",
        "KSlN3yHqYklG",
        "t6dVpIINYklI",
        "ijmpgYnKYklI",
        "-JiQyfWJYklI",
        "EM7whBJCYoAo",
        "fge-S5ZAYoAp",
        "85gYPyotYoAp",
        "RoGjAbkUYoAp",
        "4Of9eVA-YrdM",
        "iky9q4vBYrdO",
        "F6T5p64dYrdO",
        "y-Ehk30pYrdP",
        "bamQiAODYuh1",
        "QHF8YVU7Yuh3",
        "GwzvFGzlYuh3",
        "qYpmQ266Yuh3",
        "OH-pJp9IphqM",
        "bbFf2-_FphqN",
        "_ouA3fa0phqN",
        "Seke61FWphqN",
        "PIIx-8_IphqN",
        "t27r6nlMphqO",
        "r2jJGEOYphqO",
        "b0JNsNcRphqO",
        "BZR9WyysphqO",
        "jj7wYXLtphqO",
        "eZrbJ2SmphqO",
        "rFu4xreNphqO",
        "YJ55k-q6phqO",
        "gCFgpxoyphqP",
        "OVtJsKN_phqQ",
        "lssrdh5qphqQ",
        "U2RJ9gkRphqQ",
        "1M8mcRywphqQ",
        "tgIPom80phqQ",
        "JMzcOPDDphqR",
        "x-EpHcCOp1ci",
        "X_VqEhTip1ck",
        "8zGJKyg5p1ck",
        "PVzmfK_Ep1ck",
        "n3dbpmDWp1ck",
        "ylSl6qgtp1ck",
        "ZWILFDl5p1ck",
        "M7G43BXep1ck",
        "Ag9LCva-p1cl",
        "E6MkPsBcp1cl",
        "2cELzS2fp1cl",
        "3MPXvC8up1cl",
        "NC_X3p0fY2L0",
        "UV0SzAkaZNRQ",
        "YPEH6qLeZNRQ",
        "q29F0dvdveiT",
        "EXh0U9oCveiU",
        "22aHeOlLveiV",
        "g-ATYxFrGrvw",
        "Yfr_Vlr8HBkt",
        "8yEUt7NnHlrM",
        "tEA2Xm5dHt1r",
        "I79__PHVH19G",
        "Ou-I18pAyIpj",
        "fF3858GYyt-u",
        "4_0_7-oCpUZd",
        "hwyV_J3ipUZe",
        "3yB-zSqbpUZe",
        "dEUvejAfpUZe",
        "Fd15vwWVpUZf",
        "bn_IUdTipZyH",
        "49K5P_iCpZyH",
        "Nff-vKELpZyI",
        "kLW572S8pZyI",
        "dWbDXHzopZyI",
        "yLjJCtPM0KBk",
        "xiyOF9F70UgQ",
        "7wuGOrhz0itI",
        "id1riN9m0vUs",
        "578E2V7j08f6",
        "89xtkJwZ18nB",
        "67NQN5KX2AMe",
        "Iwf50b-R2tYG",
        "GMQiZwjn3iu7",
        "WVIkgGqN3qsr",
        "XkPnILGE3zoT",
        "Hlsf0x5436Go",
        "mT9DMSJo4nBL",
        "c49ITxTc407N",
        "OeJFEK0N496M",
        "9ExmJH0g5HBk",
        "cJNqERVU536h",
        "k5UmGsbsOxih",
        "T0VqWOYE6DLQ",
        "qBMux9mC6MCf",
        "-oLEiFgy-5Pf",
        "C74aWNz2AliB",
        "2DejudWSA-a0",
        "pEMng2IbBLp7",
        "rAdphbQ9Bhjc",
        "TNVZ9zx19K6k",
        "nqoHp30x9hH9",
        "rMDnDkt2B6du",
        "yiiVWRdJDDil",
        "1UUpS68QDMuG",
        "kexQrXU-DjzY",
        "T5CmagL3EC8N",
        "BhH2vgX9EjGr",
        "qjKvONjwE8ra",
        "P1XJ9OREExlT",
        "VFOzZv6IFROw",
        "TIqpNgepFxVj",
        "VfCC591jGiD4",
        "OB4l2ZhMeS1U",
        "ArJBuiUVfxKd",
        "4qY1EAkEfxKe",
        "PiV4Ypx8fxKe",
        "TfvqoZmBfxKf",
        "dJ2tPlVmpsJ0",
        "JWYfwnehpsJ1",
        "-jK_YjpMpsJ2",
        "HAih1iBOpsJ2",
        "zVGeBEFhpsJ2",
        "bmKjuQ-FpsJ3",
        "Fze-IPXLpx6K",
        "7AN1z2sKpx6M",
        "9PIHJqyupx6M",
        "_-qAgymDpx6N",
        "Z-hykwinpx6N",
        "h_CCil-SKHpo",
        "cBFFvTBNJzUa",
        "HvGl1hHyA_VK",
        "EyNgTHvd2WFk",
        "KH5McJBi2d8v",
        "iW_Lq9qf2h6X",
        "-Kee-DAl2viO",
        "gCX9965dhzqZ",
        "gIfDvo9L0UH2"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prashanth1009/Assignment_1/blob/main/Another_copy_of_Sample_ML_Submission_Template.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Name**    - PaisaBazaar"
      ],
      "metadata": {
        "id": "vncDsAP0Gaoa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Project Type**    - EDA/Regression/Classification/Unsupervised\n",
        "##### **Contribution**    - Individual/Team\n",
        "##### **Team Member 1 -** A.Prashanth\n",
        "##### **Team Member 2 -**\n",
        "##### **Team Member 3 -**\n",
        "##### **Team Member 4 -**"
      ],
      "metadata": {
        "id": "beRrZCGUAJYm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Project Summary -**"
      ],
      "metadata": {
        "id": "FJNUwmbgGyua"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The PaisaBazaar project focuses on creating a data-driven financial marketplace platform aimed at simplifying the process of comparing, applying for, and managing financial products such as loans, credit cards, and insurance. The platform serves as a bridge between financial institutions and consumers, providing personalized recommendations using user data, credit scores, and financial behavior.\n",
        "\n",
        "The primary objective of the project is to enhance user experience by enabling smarter, faster, and more transparent decision-making when selecting financial products. The platform leverages artificial intelligence and machine learning algorithms to analyze customer data, assess creditworthiness, and match users with suitable products. Real-time eligibility checks, document upload features, and application tracking systems are integrated to reduce manual effort and improve the turnaround time for loan approvals or credit card issuances.\n",
        "\n",
        "Key components of the project include a secure user registration system, integration with multiple banks and financial institutions through APIs, a personalized dashboard, and a recommendation engine. Backend systems are built to handle large volumes of financial data, ensure compliance with regulatory standards (such as KYC and GDPR), and provide robust analytics dashboards for business decision-making.\n",
        "\n",
        "The project also addresses challenges such as fraud detection, data privacy, and customer onboarding. Predictive analytics models are used to detect anomalies and reduce risk, while secure encryption protocols ensure data safety.\n",
        "\n",
        "From a business perspective, the PaisaBazaar platform increases customer acquisition for partner banks, enhances customer retention through value-added services like credit score tracking, and offers monetization through lead generation and premium services.\n",
        "\n",
        "In conclusion, the PaisaBazaar project delivers a comprehensive, scalable, and user-centric financial platform that empowers customers to make informed financial decisions while providing financial institutions with a robust lead management and analytics ecosystem. It exemplifies the use of fintech innovation to promote financial inclusion and digital transformation in the personal finance sector."
      ],
      "metadata": {
        "id": "F6v_1wHtG2nS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **GitHub Link -**"
      ],
      "metadata": {
        "id": "w6K7xa23Elo4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/SSubhashReddy/AI-ML-project/tree/main"
      ],
      "metadata": {
        "id": "h1o69JH3Eqqn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Problem Statement**\n"
      ],
      "metadata": {
        "id": "yQaldy8SH6Dl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In today’s fast-paced financial ecosystem, individuals face significant challenges in selecting suitable financial products due to a lack of transparency, overwhelming options, and limited access to personalized financial advice. Traditional banking systems often involve lengthy application processes, complex documentation, and unclear eligibility criteria, leading to poor user experiences and high rejection rates. Consumers struggle to compare offerings like personal loans, credit cards, and insurance plans across various institutions in one place. On the other hand, financial institutions face difficulties in reaching the right target audience, resulting in inefficient lead generation and high customer acquisition costs. There is a pressing need for a unified digital platform that simplifies product comparison, enables real-time eligibility checks, and offers personalized financial recommendations. The PaisaBazaar project aims to address these issues by building a data-driven fintech marketplace that connects consumers and financial institutions, ensuring faster decisions, enhanced transparency, and improved financial access for all users.\n"
      ],
      "metadata": {
        "id": "DpeJGUA3kjGy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **General Guidelines** : -  "
      ],
      "metadata": {
        "id": "mDgbUHAGgjLW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.   Well-structured, formatted, and commented code is required.\n",
        "2.   Exception Handling, Production Grade Code & Deployment Ready Code will be a plus. Those students will be awarded some additional credits.\n",
        "     \n",
        "     The additional credits will have advantages over other students during Star Student selection.\n",
        "       \n",
        "             [ Note: - Deployment Ready Code is defined as, the whole .ipynb notebook should be executable in one go\n",
        "                       without a single error logged. ]\n",
        "\n",
        "3.   Each and every logic should have proper comments.\n",
        "4. You may add as many number of charts you want. Make Sure for each and every chart the following format should be answered.\n",
        "        \n",
        "\n",
        "```\n",
        "# Chart visualization code\n",
        "```\n",
        "            \n",
        "\n",
        "*   Why did you pick the specific chart?\n",
        "*   What is/are the insight(s) found from the chart?\n",
        "* Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason.\n",
        "\n",
        "5. You have to create at least 15 logical & meaningful charts having important insights.\n",
        "\n",
        "\n",
        "[ Hints : - Do the Vizualization in  a structured way while following \"UBM\" Rule.\n",
        "\n",
        "U - Univariate Analysis,\n",
        "\n",
        "B - Bivariate Analysis (Numerical - Categorical, Numerical - Numerical, Categorical - Categorical)\n",
        "\n",
        "M - Multivariate Analysis\n",
        " ]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "6. You may add more ml algorithms for model creation. Make sure for each and every algorithm, the following format should be answered.\n",
        "\n",
        "\n",
        "*   Explain the ML Model used and it's performance using Evaluation metric Score Chart.\n",
        "\n",
        "\n",
        "*   Cross- Validation & Hyperparameter Tuning\n",
        "\n",
        "*   Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart.\n",
        "\n",
        "*   Explain each evaluation metric's indication towards business and the business impact pf the ML model used.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ZrxVaUj-hHfC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ***Let's Begin !***"
      ],
      "metadata": {
        "id": "O_i_v8NEhb9l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***1. Know Your Data***"
      ],
      "metadata": {
        "id": "HhfV-JJviCcP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import Libraries"
      ],
      "metadata": {
        "id": "Y3lxredqlCYt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import Libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "M8Vqi-pPk-HR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Loading"
      ],
      "metadata": {
        "id": "3RnN4peoiCZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load Dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4CkvbW_SlZ_R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "try:\n",
        "    # Attempt to read the specified CSV file into a DataFrame from Google Drive\n",
        "    # Changed from pd.read_excel to pd.read_csv as the file extension is .csv\n",
        "    df = pd.read_csv('/content/drive/MyDrive/dataset-2.csv')\n",
        "except FileNotFoundError:\n",
        "    # If the file is not found, print a specific error message mentioning the correct filename and path\n",
        "    print(\"Error: The file '/content/drive/MyDrive/dataset-2.csv' was not found.\")\n",
        "    print(\"Please verify the file path and ensure the file exists and is correctly named in your Google Drive.\")"
      ],
      "metadata": {
        "id": "fUNOfkL9ws3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset First View"
      ],
      "metadata": {
        "id": "x71ZqKXriCWQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset First Look\n",
        "df.head()"
      ],
      "metadata": {
        "id": "LWNFOSvLl09H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Rows & Columns count"
      ],
      "metadata": {
        "id": "7hBIi_osiCS2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Rows & Columns count\n",
        "df.shape"
      ],
      "metadata": {
        "id": "Kllu7SJgmLij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dataset Information"
      ],
      "metadata": {
        "id": "JlHwYmJAmNHm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Info\n",
        "df.info()"
      ],
      "metadata": {
        "id": "e9hRXRi6meOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Duplicate Values"
      ],
      "metadata": {
        "id": "35m5QtbWiB9F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Duplicate Value Count\n",
        "df.duplicated().sum()"
      ],
      "metadata": {
        "id": "1sLdpKYkmox0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Missing Values/Null Values"
      ],
      "metadata": {
        "id": "PoPl-ycgm1ru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Missing Values/Null Values Count\n",
        "df.isnull().sum()\n"
      ],
      "metadata": {
        "id": "GgHWkxvamxVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing the missing values\n",
        "import matplotlib.pyplot as plt # Ensure plt is imported\n",
        "import seaborn as sns # Ensure seaborn is imported\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df.isnull(), cmap='viridis', cbar=False)\n",
        "plt.title('Missing Values Heatmap')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "3q5wnI3om9sJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What did you know about your dataset?"
      ],
      "metadata": {
        "id": "H0kj-8xxnORC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It contains financial data like customer info, loan details, and credit scores.\n",
        "\n",
        "Focuses on loan applications, approvals, and product preferences.\n",
        "\n",
        "Useful for analyzing customer behavior, credit risk, and predicting loan approval.\n",
        "\n",
        "Helps in targeted marketing and financial product insights."
      ],
      "metadata": {
        "id": "gfoNAAC-nUe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***2. Understanding Your Variables***"
      ],
      "metadata": {
        "id": "nA9Y7ga8ng1Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Columns\n",
        "df.columns"
      ],
      "metadata": {
        "id": "j7xfkqrt5Ag5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset Describe\n",
        "df.describe()"
      ],
      "metadata": {
        "id": "DnOaZdaE5Q5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Variables Description"
      ],
      "metadata": {
        "id": "PBTbrJXOngz2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Customer ID:** A unique identifier given to each customer.\n",
        "\n",
        "**Age:**The age of the customer applying for a loan or service.\n",
        "\n",
        "**Gender:** Specifies whether the customer is male or female.\n",
        "\n",
        "**Location:** The city or region where the customer resides.\n",
        "\n",
        "**Income:** The monthly income of the customer.\n",
        "\n",
        "**Employment Type:** Indicates if the customer is salaried or self-employed.\n",
        "\n",
        "**Loan Type:** The category of loan applied for, like personal loan or home loan.\n",
        "\n",
        "**Loan Amount:** The amount of money the customer wants to borrow.\n",
        "\n",
        "**Tenure:** The loan repayment period in months.\n",
        "\n",
        "**Interest Rate:** The rate of interest charged on the loan.\n",
        "\n",
        "**CIBIL Score:** The credit score showing the customer’s creditworthiness.\n",
        "\n",
        "**Application Status:** Shows whether the loan is approved, rejected, or pending.\n",
        "\n",
        "**Application Date:** The date on which the customer applied for the loan."
      ],
      "metadata": {
        "id": "aJV4KIxSnxay"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Check Unique Values for each variable."
      ],
      "metadata": {
        "id": "u3PMJOP6ngxN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check Unique Values for each variable\n",
        "df.nunique()"
      ],
      "metadata": {
        "id": "7MITJj9y1bo1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. ***Data Wrangling***"
      ],
      "metadata": {
        "id": "dauF4eBmngu3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Wrangling Code"
      ],
      "metadata": {
        "id": "bKJF3rekwFvQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Write your code to make your dataset analysis ready.\n",
        "df_copy = df.copy()\n",
        "\n",
        "#drop unnecessary columns\n",
        "# Use errors='ignore' to avoid KeyError if a column is not found\n",
        "drop_columns = ['ID', 'Customer_ID', 'Name', 'SSN']\n",
        "df.drop(columns = drop_columns, inplace = True, errors='ignore')\n",
        "\n",
        "#convert data types\n",
        "# Check if the columns exist before converting their types\n",
        "if 'Num_Bank_Accounts' in df.columns:\n",
        "    df['Num_Bank_Accounts'] = df['Num_Bank_Accounts'].astype('int64')\n",
        "if 'Age' in df.columns:\n",
        "    df['Age'] = df['Age'].astype('int64')\n",
        "if 'Num_Credit_Inquiries' in df.columns:\n",
        "    df['Num_Credit_Inquiries'] = df['Num_Credit_Inquiries'].astype('int64')\n",
        "\n",
        "#round numerical values\n",
        "df = df.round(2)"
      ],
      "metadata": {
        "id": "wk-9a2fpoLcV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#feature engineering\n",
        "#1. Debt to income ratio\n",
        "df['Debt_to_Income_Ratio'] = df['Outstanding_Debt'] / df['Annual_Income']\n",
        "\n",
        "#2. Credit card Utilization score\n",
        "df['Credit_Card_Utilization_Score'] = df['Credit_Utilization_Ratio'] * df['Num_Credit_Card']\n",
        "\n",
        "#3. Credit Mix score\n",
        "credit_mix_mapping = {'Bad': 0, 'Standard': 1, 'Good': 2}\n",
        "df['Credit_Mix_Score'] = df['Credit_Mix'].map(credit_mix_mapping)\n",
        "\n",
        "\n",
        "#4. Payment Delay Score\n",
        "df['Payment_Delay_Score'] = df['Num_of_Delayed_Payment'] * df['Delay_from_due_date']"
      ],
      "metadata": {
        "id": "nAP68aXlzo_T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What all manipulations have you done and insights you found?"
      ],
      "metadata": {
        "id": "MSa1f5Uengrz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Manipulations :**\n",
        "\n",
        "Removed Irrelevant Columns\n",
        "\n",
        "Data type conversion\n",
        "\n",
        "Rounded Numerical Values\n",
        "\n",
        "Feature Engineering\n",
        "\n",
        "**Insights found:**\n",
        "\n",
        "better data quality\n",
        "\n",
        "Impact of Debt to Income Ratio\n",
        "\n",
        "Credit Utillisation and Risk\n",
        "\n",
        "Delayed Payment Behaviou"
      ],
      "metadata": {
        "id": "LbyXE7I1olp8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***4. Data Vizualization, Storytelling & Experimenting with charts : Understand the relationships between variables***"
      ],
      "metadata": {
        "id": "GF8Ens_Soomf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 1"
      ],
      "metadata": {
        "id": "0wOQAZs5pc--"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 1 visualization code\n",
        "sns.countplot(x = df['Credit_Score'], hue = df['Credit_Score'], palette = 'viridis', order = df['Credit_Score'].value_counts().index)\n",
        "#Set labels and title\n",
        "plt.title('Distribution of Credit Scores')\n",
        "plt.xlabel('Credit Score Category')\n",
        "plt.ylabel('Count')\n",
        "#show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7v_ESjsspbW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "K5QZ13OEpz2H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A bar chart clearly shows and compares the number of people in each credit score category."
      ],
      "metadata": {
        "id": "XESiWehPqBRc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "lQ7QKXXCp7Bj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most people have Standard credit scores.\n",
        "\n",
        "A large group has Poor scores.\n",
        "\n",
        "Few have Good scores."
      ],
      "metadata": {
        "id": "C_j1G7yiqdRP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "448CDAPjqfQr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive business impact:**\n",
        "\n",
        "Target credit improvement tools to Poor/Standard groups.\n",
        "\n",
        "Offer rewards to retain Good score customers.\n",
        "\n",
        "**Negative insight:**\n",
        "\n",
        "High number of Poor scores may lead to more defaults and reduced loan approvals, which can hurt business."
      ],
      "metadata": {
        "id": "3cspy4FjqxJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 2"
      ],
      "metadata": {
        "id": "KSlN3yHqYklG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 2 visualization code\n",
        "sns.histplot(df['Age'], bins = 30, kde = True, color = 'blue')\n",
        "\n",
        "#set labels and title\n",
        "plt.title('Distribution of Age')\n",
        "plt.xlabel('Age')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "#show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "R4YgtaqtYklH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t6dVpIINYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histogram with KDE line is perfect to show the age distribution and spot trends easily."
      ],
      "metadata": {
        "id": "5aaW0BYyYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ijmpgYnKYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most people are aged 20–45.\n",
        "\n",
        "Fewer users are below 18 or above 50."
      ],
      "metadata": {
        "id": "PSx9atu2YklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "-JiQyfWJYklI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive business impact:**\n",
        "\n",
        "Helps target products/services to the 20–45 age group, the largest audience.\n",
        "\n",
        "**Negative insight:**\n",
        "\n",
        "Low presence of older users may mean missed opportunities if their needs are ignored."
      ],
      "metadata": {
        "id": "BcBbebzrYklV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 3"
      ],
      "metadata": {
        "id": "EM7whBJCYoAo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 3 visualization code\n",
        "sns.histplot(df['Annual_Income'], bins = 30, kde = True, color = 'green')\n",
        "\n",
        "#set labels and title\n",
        "plt.title('Distribution of Annual Income')\n",
        "plt.xlabel('Annual Income')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "#show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t6GMdE67YoAp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "fge-S5ZAYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A histogram with KDE is ideal to show how annual income is distributed across the population."
      ],
      "metadata": {
        "id": "5dBItgRVYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "85gYPyotYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most people earn between ₹10,000 – ₹40,000 annually.\n",
        "\n",
        "Very few earn above ₹1,00,000, showing a right-skewed distribution."
      ],
      "metadata": {
        "id": "4jstXR6OYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "RoGjAbkUYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Positive business impact:**\n",
        "\n",
        "Helps businesses focus products or pricing for low-to-middle income groups, which are the majority.\n",
        "\n",
        "**Negative insight:**\n",
        "\n",
        "Smaller high-income segment may mean limited market for premium services or luxury products."
      ],
      "metadata": {
        "id": "zfJ8IqMcYoAp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 4"
      ],
      "metadata": {
        "id": "4Of9eVA-YrdM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 4 visualization code\n",
        "sns.histplot(df['Credit_Utilization_Ratio'], bins = 30, kde = True, color = 'purple')\n",
        "\n",
        "#set labels and title\n",
        "plt.title('Distribution of Credit Utilization Ratio')\n",
        "plt.xlabel('Credit Utilization Ratio')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "#show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "irlUoxc8YrdO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "iky9q4vBYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This histogram with KDE (Kernel Density Estimate) line was chosen to visualize the distribution of Credit Utilization Ratio, making it easy to observe data concentration and shape (e.g., normality, skewness)."
      ],
      "metadata": {
        "id": "aJRCwT6DYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "F6T5p64dYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distribution is nearly bell-shaped and symmetric, peaking between 30–35%.\n",
        "\n",
        "Most credit utilization values lie between 25% and 40%.\n",
        "\n",
        "Very few customers have ratios below 25% or above 45%."
      ],
      "metadata": {
        "id": "Xx8WAJvtYrdO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "y-Ehk30pYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Identify typical usage behavior for better risk scoring.\n",
        "\n",
        "Tailor credit products for the majority who maintain healthy utilization (30–35%)."
      ],
      "metadata": {
        "id": "jLNxxz7MYrdP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 5"
      ],
      "metadata": {
        "id": "bamQiAODYuh1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 5 visualization code\n",
        "sns.histplot(df['Num_Credit_Card'], bins = 15, kde = True, color = 'red')\n",
        "\n",
        "#set labels and title\n",
        "plt.title('Distribution of Number of Credit Cards')\n",
        "plt.xlabel('Number of Credit Cards')\n",
        "plt.ylabel('Frequency')\n",
        "\n",
        "#show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIJwrbroYuh3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "QHF8YVU7Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This bar chart with KDE line was chosen to understand the distribution pattern of the number of credit cards held by users, helping identify customer segments and behaviors."
      ],
      "metadata": {
        "id": "dcxuIMRPYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "GwzvFGzlYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most users have 4 to 7 credit cards, with peaks at 5 and 6.\n",
        "\n",
        "Very few users have 1 or fewer or more than 9 cards.\n",
        "\n",
        "The KDE line indicates some irregular fluctuations, possibly due to grouped or rounded data."
      ],
      "metadata": {
        "id": "uyqkiB8YYuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "qYpmQ266Yuh3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Credit companies can target promotions to users with 3–7 cards—this is the most active group.\n",
        "\n",
        "It supports portfolio expansion strategies or credit limit increases."
      ],
      "metadata": {
        "id": "_WtzZ_hCYuh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 6"
      ],
      "metadata": {
        "id": "OH-pJp9IphqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 6 visualization code\n",
        "sns.boxplot(x= 'Credit_Score', y = \"Annual_Income\", data = df, palette = 'viridis')\n",
        "\n",
        "#set label and title\n",
        "plt.title('Annual Income vs Credit Score')\n",
        "plt.xlabel('Credit Score')\n",
        "plt.ylabel('Annual Income')\n",
        "\n",
        "#show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "kuRf4wtuphqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "bbFf2-_FphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Credit companies can target promotions to users with 3–7 cards—this is the most active group.\n",
        "\n",
        "It supports portfolio expansion strategies or credit limit increases."
      ],
      "metadata": {
        "id": "loh7H2nzphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "_ouA3fa0phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most users have 4 to 7 credit cards, with peaks at 5 and 6.\n",
        "\n",
        "Very few users have 1 or fewer or more than 9 cards.\n",
        "\n",
        "The KDE line indicates some irregular fluctuations, possibly due to grouped or rounded data."
      ],
      "metadata": {
        "id": "VECbqPI7phqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "Seke61FWphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Credit companies can target promotions to users with 3–7 cards—this is the most active group.\n",
        "\n",
        "It supports portfolio expansion strategies or credit limit increases.\n",
        "\n"
      ],
      "metadata": {
        "id": "DW4_bGpfphqN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 7"
      ],
      "metadata": {
        "id": "PIIx-8_IphqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 7 visualization code\n",
        "# Assuming 'Payment_Behaviour' and 'Credit_Score' are relevant columns\n",
        "# If they don't exist, choose other categorical and numerical columns from your df.columns list\n",
        "if 'Payment_Behaviour' in df.columns and 'Credit_Score' in df.columns:\n",
        "    sns.boxplot(x='Payment_Behaviour', y='Credit_Score', data=df, palette='magma')\n",
        "\n",
        "    # Set labels and title\n",
        "    plt.title('Credit Score vs Payment Behaviour')\n",
        "    plt.xlabel('Payment Behaviour')\n",
        "    plt.ylabel('Credit Score')\n",
        "\n",
        "    # Rotate x-axis labels if they overlap\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "    # Show plot\n",
        "    plt.tight_layout() # Adjust layout to prevent labels overlapping\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Required columns ('Payment_Behaviour' or 'Credit_Score') not found in the DataFrame.\")\n",
        "    print(\"Available columns:\", df.columns.tolist())"
      ],
      "metadata": {
        "id": "lqAIGUfyphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "t27r6nlMphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This box plot was chosen to compare the distribution of credit scores across different payment behaviours, helping to visualize median values, spread, and outliers effectively."
      ],
      "metadata": {
        "id": "iv6ro40sphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "r2jJGEOYphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Across all categories, the median credit score remains close to \"Standard\".\n",
        "\n",
        "Slightly higher scores are seen in users with high spend & large value payments.\n",
        "\n",
        "Low spend with medium or small payments shows more users with lower credit scores and greater spread."
      ],
      "metadata": {
        "id": "Po6ZPi4hphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "b0JNsNcRphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helps segment customers by behavior and design custom repayment plans or credit offerings.\n",
        "\n",
        "Encourages positive payment patterns like large-value payments, which may be linked to better credit scores."
      ],
      "metadata": {
        "id": "xvSq8iUTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 8"
      ],
      "metadata": {
        "id": "BZR9WyysphqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 8 visualization code\n",
        "# Let's explore the relationship between Loan Amount and Credit Score\n",
        "# Using a boxplot to compare the distribution of Loan Amount across Credit Score categories\n",
        "\n",
        "# Check if the necessary columns exist\n",
        "if 'Loan Amount' in df.columns and 'Credit_Score' in df.columns:\n",
        "    sns.boxplot(x='Credit_Score', y='Loan Amount', data=df, palette='pastel')\n",
        "\n",
        "    # Set labels and title\n",
        "    plt.title('Loan Amount vs Credit Score')\n",
        "    plt.xlabel('Credit Score')\n",
        "    plt.ylabel('Loan Amount')\n",
        "\n",
        "    # Show plot\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Required columns ('Loan Amount' or 'Credit_Score') not found in the DataFrame.\")\n",
        "    print(\"Available columns:\", df.columns.tolist())"
      ],
      "metadata": {
        "id": "TdPTWpAVphqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "jj7wYXLtphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The box plot was selected to clearly show the variation in credit score across different payment behaviours, helping compare median scores, ranges, and outliers."
      ],
      "metadata": {
        "id": "Ob8u6rCTphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "eZrbJ2SmphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most payment behaviours result in standard credit scores.\n",
        "\n",
        "High spend with large payments tends to maintain higher median credit scores.\n",
        "\n",
        "Low spend with small/medium payments shows more poor score outliers."
      ],
      "metadata": {
        "id": "mZtgC_hjphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "rFu4xreNphqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Businesses can promote good payment habits (e.g., larger consistent payments) to improve customer credit health.\n",
        "\n",
        "Helps design risk-based offerings based on spending and payment style."
      ],
      "metadata": {
        "id": "ey_0qi68phqO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 9"
      ],
      "metadata": {
        "id": "YJ55k-q6phqO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 9 visualization code\n",
        "sns.boxplot(x= 'Credit_Score', y = \"Annual_Income\", data = df, palette = 'viridis')\n",
        "\n",
        "#set label and title\n",
        "plt.title('Annual Income vs Credit Score')\n",
        "plt.xlabel('Credit Score')\n",
        "plt.ylabel('Annual Income')\n",
        "\n",
        "#show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "B2aS4O1ophqO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "gCFgpxoyphqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This box plot effectively shows how annual income varies across credit score categories, helping to identify income patterns linked to creditworthiness."
      ],
      "metadata": {
        "id": "TVxDimi2phqP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "OVtJsKN_phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Users with Good credit scores generally have higher median and wider income ranges.\n",
        "\n",
        "Poor credit scores are associated with lower median incomes and more lower-income outliers.\n",
        "\n",
        "Outliers exist in all groups, but high earners are more common in the Good score group."
      ],
      "metadata": {
        "id": "ngGi97qjphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "lssrdh5qphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helps in credit risk assessment and income-based segmentation.\n",
        "\n",
        "High-income, good-score customers can be targeted for premium credit products."
      ],
      "metadata": {
        "id": "tBpY5ekJphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 10"
      ],
      "metadata": {
        "id": "U2RJ9gkRphqQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 10 visualization code\n",
        "# Let's visualize the relationship between Age and Annual Income using a scatter plot\n",
        "# with Credit Score as the hue to see how age and income relate to creditworthiness.\n",
        "\n",
        "# Check if the necessary columns exist\n",
        "if 'Age' in df.columns and 'Annual_Income' in df.columns and 'Credit_Score' in df.columns:\n",
        "    sns.scatterplot(x='Age', y='Annual_Income', hue='Credit_Score', data=df, palette='viridis', alpha=0.6)\n",
        "\n",
        "    # Set labels and title\n",
        "    plt.title('Age vs Annual Income by Credit Score')\n",
        "    plt.xlabel('Age')\n",
        "    plt.ylabel('Annual Income')\n",
        "\n",
        "    # Show plot\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Required columns ('Age', 'Annual_Income', or 'Credit_Score') not found in the DataFrame.\")\n",
        "    print(\"Available columns:\", df.columns.tolist())"
      ],
      "metadata": {
        "id": "GM7a4YP4phqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "1M8mcRywphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This scatter plot was chosen to visualize the relationship between age, income, and credit score, enabling multi-variable comparison across customer segments."
      ],
      "metadata": {
        "id": "8agQvks0phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "tgIPom80phqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Good credit scores (dark blue) are more common in higher income brackets across all age groups.\n",
        "\n",
        "Poor scores (light green) are more frequent in the lower income range, regardless of age.\n",
        "\n",
        "There's no strong age-income dependency—high incomes exist across ages."
      ],
      "metadata": {
        "id": "Qp13pnNzphqQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "JMzcOPDDphqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helps target financially stable customers for credit offers, regardless of age.\n",
        "\n",
        "Useful for creditworthiness prediction models based on income and score clusters."
      ],
      "metadata": {
        "id": "R4Ka1PC2phqR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 11"
      ],
      "metadata": {
        "id": "x-EpHcCOp1ci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 11 visualization code\n",
        "# Let's visualize the relationship between Loan Amount and Interest Rate\n",
        "# using a scatter plot, potentially colored by Credit Score for more insight.\n",
        "\n",
        "# Check if the necessary columns exist\n",
        "if 'Loan Amount' in df.columns and 'Interest Rate' in df.columns and 'Credit_Score' in df.columns:\n",
        "    # Chart - 11 visualization code\n",
        "    sns.scatterplot(x='Loan Amount', y='Interest Rate', hue='Credit_Score', data=df, palette='plasma', alpha=0.6)\n",
        "\n",
        "    # Set labels and title\n",
        "    plt.title('Loan Amount vs Interest Rate by Credit Score')\n",
        "    plt.xlabel('Loan Amount')\n",
        "    plt.ylabel('Interest Rate')\n",
        "\n",
        "    # Show plot\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Required columns ('Loan Amount', 'Interest Rate', or 'Credit_Score') not found in the DataFrame.\")\n",
        "    print(\"Available columns:\", df.columns.tolist())"
      ],
      "metadata": {
        "id": "mAQTIvtqp1cj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "X_VqEhTip1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The box plot was selected to clearly show the variation in credit score across different payment behaviours, helping compare median scores, ranges, and outliers."
      ],
      "metadata": {
        "id": "-vsMzt_np1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "8zGJKyg5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most payment behaviours result in standard credit scores.\n",
        "\n",
        "High spend with large payments tends to maintain higher median credit scores.\n",
        "\n",
        "Low spend with small/medium payments shows more poor score outliers."
      ],
      "metadata": {
        "id": "ZYdMsrqVp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "PVzmfK_Ep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Businesses can promote good payment habits (e.g., larger consistent payments) to improve customer credit health.\n",
        "\n",
        "Helps design risk-based offerings based on spending and payment style."
      ],
      "metadata": {
        "id": "druuKYZpp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 12"
      ],
      "metadata": {
        "id": "n3dbpmDWp1ck"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 12 visualization code\n",
        "# Let's visualize the relationship between Number of Credit Inquiries and Credit Score\n",
        "# Using a boxplot to compare the distribution of inquiries across Credit Score categories\n",
        "\n",
        "# Check if the necessary columns exist\n",
        "if 'Num_Credit_Inquiries' in df.columns and 'Credit_Score' in df.columns:\n",
        "    sns.boxplot(x='Credit_Score', y='Num_Credit_Inquiries', data=df, palette='viridis')\n",
        "\n",
        "    # Set labels and title\n",
        "    plt.title('Number of Credit Inquiries vs Credit Score')\n",
        "    plt.xlabel('Credit Score')\n",
        "    plt.ylabel('Number of Credit Inquiries')\n",
        "\n",
        "    # Show plot\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Required columns ('Num_Credit_Inquiries' or 'Credit_Score') not found in the DataFrame.\")\n",
        "    print(\"Available columns:\", df.columns.tolist())"
      ],
      "metadata": {
        "id": "bwevp1tKp1ck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "ylSl6qgtp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This box plot was chosen to show the link between credit inquiries and credit score, which helps assess financial behavior and risk."
      ],
      "metadata": {
        "id": "m2xqNkiQp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "ZWILFDl5p1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Users with Good credit scores have fewer credit inquiries.\n",
        "\n",
        "Poor credit scores are linked to a higher number of inquiries.\n",
        "\n",
        "More inquiries often reflect credit-seeking behavior or risk."
      ],
      "metadata": {
        "id": "x-lUsV2mp1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "M7G43BXep1ck"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Helps in building credit risk models—fewer inquiries may signal more stable customers.\n",
        "\n",
        "Supports pre-approval targeting for low-inquiry users."
      ],
      "metadata": {
        "id": "5wwDJXsLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 13"
      ],
      "metadata": {
        "id": "Ag9LCva-p1cl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chart - 13 visualization code\n",
        "# Let's visualize the relationship between Number of Open Accounts and Credit Score\n",
        "# Using a boxplot to compare the distribution of open accounts across Credit Score categories\n",
        "\n",
        "# Check if the necessary columns exist\n",
        "if 'Num_Open_Accounts' in df.columns and 'Credit_Score' in df.columns:\n",
        "    sns.boxplot(x='Credit_Score', y='Num_Open_Accounts', data=df, palette='viridis')\n",
        "\n",
        "    # Set labels and title\n",
        "    plt.title('Number of Open Accounts vs Credit Score')\n",
        "    plt.xlabel('Credit Score')\n",
        "    plt.ylabel('Number of Open Accounts')\n",
        "\n",
        "    # Show plot\n",
        "    plt.show()\n",
        "else:\n",
        "    print(\"Required columns ('Num_Open_Accounts' or 'Credit_Score') not found in the DataFrame.\")\n",
        "    print(\"Available columns:\", df.columns.tolist())"
      ],
      "metadata": {
        "id": "EUfxeq9-p1cl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "E6MkPsBcp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The box plot was selected to clearly show the variation in credit score across different payment behaviours, helping compare median scores, ranges, and outliers."
      ],
      "metadata": {
        "id": "V22bRsFWp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "2cELzS2fp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most payment behaviours result in standard credit scores.\n",
        "\n",
        "High spend with large payments tends to maintain higher median credit scores.\n",
        "\n",
        "Low spend with small/medium payments shows more poor score outliers."
      ],
      "metadata": {
        "id": "ozQPc2_Ip1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 3. Will the gained insights help creating a positive business impact?\n",
        "Are there any insights that lead to negative growth? Justify with specific reason."
      ],
      "metadata": {
        "id": "3MPXvC8up1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Businesses can promote good payment habits (e.g., larger consistent payments) to improve customer credit health.\n",
        "\n",
        "Helps design risk-based offerings based on spending and payment style."
      ],
      "metadata": {
        "id": "GL8l1tdLp1cl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 14 - Correlation Heatmap"
      ],
      "metadata": {
        "id": "NC_X3p0fY2L0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Correlation Heatmap visualization code\n",
        "\n",
        "# Select only numerical columns for the correlation matrix\n",
        "numerical_cols = df.select_dtypes(include=np.number).columns\n",
        "correlation_matrix = df[numerical_cols].corr()\n",
        "\n",
        "# Create the heatmap\n",
        "plt.figure(figsize=(14, 10))\n",
        "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=.5)\n",
        "plt.title('Correlation Heatmap of Numerical Features')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xyC9zolEZNRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "UV0SzAkaZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The correlation heatmap is used to quickly identify strong relationships between numerical variables in the dataset, helping guide feature selection and model design."
      ],
      "metadata": {
        "id": "DVPuT8LYZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "YPEH6qLeZNRQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Strong positive correlation between Monthly_Inhand_Salary & Annual_Income (0.81), and Outstanding_Debt & Total_EMI_per_month (0.83).\n",
        "\n",
        "Strong negative correlation between Credit_Utilization_Ratio & Credit_Mix_Score (−0.69), and Payment_Delay_Score & Credit_Mix_Score (−0.76)."
      ],
      "metadata": {
        "id": "bfSqtnDqZNRR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Chart - 15 - Pair Plot"
      ],
      "metadata": {
        "id": "q29F0dvdveiT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np # Import numpy if not already imported for checking numerical types\n",
        "\n",
        "print(\"Available columns in df:\", df.columns.tolist())\n",
        "\n",
        "# Update the list of columns for pairplot based on the correct names\n",
        "# Choose a few relevant numerical columns from your available columns\n",
        "required_cols = ['Age', 'Annual_Income', 'Interest_Rate', 'Credit_Score'] # Updated list\n",
        "present_cols = [col for col in required_cols if col in df.columns]\n",
        "\n",
        "# Ensure at least two columns are present for pairplot\n",
        "if len(present_cols) >= 2:\n",
        "    # Select only numerical columns for pairplot to avoid errors with non-numeric types\n",
        "    numerical_cols_for_pairplot = df[present_cols].select_dtypes(include=np.number).columns.tolist()\n",
        "\n",
        "    # Ensure there are columns to plot after selection\n",
        "    if len(numerical_cols_for_pairplot) > 1:\n",
        "         sns.pairplot(df[numerical_cols_for_pairplot]) # Plot without hue since 'Application_Status' is not present\n",
        "         plt.show()\n",
        "    else:\n",
        "        print(\"Not enough numerical columns present for pairplot after filtering.\")\n",
        "else:\n",
        "    print(\"Required numerical columns for pairplot not found in the DataFrame or not enough columns present.\")\n",
        "    print(\"Available columns:\", df.columns.tolist())"
      ],
      "metadata": {
        "id": "o58-TEIhveiU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 1. Why did you pick the specific chart?"
      ],
      "metadata": {
        "id": "EXh0U9oCveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This pair plot (scatter matrix) was chosen to visually explore relationships and distributions between multiple numerical variables — specifically Age, Annual_Income, and Interest_Rate — all at once."
      ],
      "metadata": {
        "id": "eMmPjTByveiU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### 2. What is/are the insight(s) found from the chart?"
      ],
      "metadata": {
        "id": "22aHeOlLveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Annual Income is right-skewed, with most individuals earning below ₹100,000.\n",
        "\n",
        "Interest Rate shows a stepwise distribution, likely due to discrete loan brackets.\n",
        "\n",
        "No strong linear correlation is observed between Age and either Income or Interest Rate.\n",
        "\n",
        "Most young to mid-age users (20–40) dominate the dataset."
      ],
      "metadata": {
        "id": "uPQ8RGwHveiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***5. Hypothesis Testing***"
      ],
      "metadata": {
        "id": "g-ATYxFrGrvw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Based on your chart experiments, define three hypothetical statements from the dataset. In the next three questions, perform hypothesis testing to obtain final conclusion about the statements through your code and statistical testing."
      ],
      "metadata": {
        "id": "Yfr_Vlr8HBkt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hypothesis 1:**\n",
        "\n",
        "\"There is no significant difference in average annual income across different credit score categories.\"\n",
        "\n",
        "Test Type: One-Way ANOVA\n",
        "\n",
        "Null Hypothesis (H₀): Mean income is the same across 'Poor', 'Standard', and 'Good' credit score groups.\n",
        "\n",
        "Alternative Hypothesis (H₁): At least one group has a different mean income.\n",
        "\n",
        "**Hypothesis 2:**\n",
        "\n",
        "\"The average age of people with good credit scores is higher than those with poor credit scores.\"\n",
        "\n",
        "Test Type: Independent t-test (two-sample)\n",
        "\n",
        "H₀: Average age of 'Good' = Average age of 'Poor'\n",
        "\n",
        "H₁: Average age of 'Good' > Average age of 'Poor'\n",
        "\n",
        "**Hypothesis 3:**\n",
        "\n",
        "\"Most of the population earns below ₹50,000 annually.\"\n",
        "\n",
        "Test Type: One-sample proportion z-test\n",
        "\n",
        "H₀: Proportion earning < ₹50,000 = 50%\n",
        "\n",
        "H₁: Proportion earning < ₹50,000 > 50%"
      ],
      "metadata": {
        "id": "-7MS06SUHkB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "tEA2Xm5dHt1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 1"
      ],
      "metadata": {
        "id": "8yEUt7NnHlrM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hypothesis 1:**\n",
        "Income vs Credit Score Category\n",
        "Research Hypothesis:\n",
        "There is a significant difference in the average annual income across different credit score categories.\n",
        "\n",
        "**Null Hypothesis (H₀):**\n",
        "There is no significant difference in mean annual income across credit score categories ('Poor', 'Standard', 'Good').\n",
        "\n",
        "**Alternate Hypothesis (H₁):**\n",
        "At least one credit score category has a significantly different mean annual income."
      ],
      "metadata": {
        "id": "HI9ZP0laH0D-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "I79__PHVH19G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value for Hypothesis 1\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Assuming 'Credit_Score' and 'Annual_Income' are columns in your DataFrame df\n",
        "\n",
        "# Separate data into groups based on Credit_Score\n",
        "poor_income = df[df['Credit_Score'] == 'Poor']['Annual_Income']\n",
        "standard_income = df[df['Credit_Score'] == 'Standard']['Annual_Income']\n",
        "good_income = df[df['Credit_Score'] == 'Good']['Annual_Income']\n",
        "\n",
        "# Perform one-way ANOVA test\n",
        "# Check if there are enough samples in each group and if the groups exist\n",
        "if len(poor_income) > 1 and len(standard_income) > 1 and len(good_income) > 1:\n",
        "    f_statistic, p_value = stats.f_oneway(poor_income, standard_income, good_income)\n",
        "\n",
        "    print(f\"One-Way ANOVA Test for Annual Income across Credit Scores:\")\n",
        "    print(f\"F-statistic: {f_statistic:.4f}\")\n",
        "    print(f\"P-value: {p_value:.4f}\")\n",
        "\n",
        "    # Interpret the results based on a significance level (e.g., 0.05)\n",
        "    alpha = 0.05\n",
        "    if p_value < alpha:\n",
        "        print(\"\\nConclusion: Reject the null hypothesis.\")\n",
        "        print(\"There is a significant difference in average annual income across different credit score categories.\")\n",
        "    else:\n",
        "        print(\"\\nConclusion: Fail to reject the null hypothesis.\")\n",
        "        print(\"There is no significant difference in average annual income across different credit score categories.\")\n",
        "else:\n",
        "    print(\"Could not perform ANOVA test. Check if the 'Credit_Score' categories exist and have more than one sample.\")"
      ],
      "metadata": {
        "id": "oZrfquKtyian"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "Ou-I18pAyIpj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used the One-Way ANOVA (Analysis of Variance) test."
      ],
      "metadata": {
        "id": "s2U0kk00ygSB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "fF3858GYyt-u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal was to compare the mean annual income across three different credit score categories: Poor, Standard, and Good.\n",
        "\n",
        "Since there are more than two independent groups and we are analyzing a numerical variable (annual income), One-Way ANOVA is the most appropriate test.\n",
        "\n",
        "ANOVA determines whether there is a statistically significant difference in means across the groups.\n",
        "\n",
        "It provides an F-statistic and a P-value to help decide if at least one group mean is different."
      ],
      "metadata": {
        "id": "HO4K0gP5y3B4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 2"
      ],
      "metadata": {
        "id": "4_0_7-oCpUZd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "hwyV_J3ipUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Research Hypothesis:**\n",
        "People with good credit scores are, on average, older than those with poor credit scores.\n",
        "\n",
        "**Null Hypothesis (H₀):**\n",
        "There is no difference or the average age of individuals with good credit scores is less than or equal to those with poor credit scores.\n",
        "→ H₀: μ_good ≤ μ_poor\n",
        "\n",
        "**Alternate Hypothesis (H₁):**\n",
        "The average age of people with good credit scores is higher than those with poor scores.\n",
        "→ H₁: μ_good > μ_poor"
      ],
      "metadata": {
        "id": "FnpLGJ-4pUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "3yB-zSqbpUZe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value for Hypothesis 2\n",
        "import scipy.stats as stats\n",
        "\n",
        "# Assuming 'Age' and 'Credit_Score' are columns in your DataFrame df\n",
        "\n",
        "# Separate age data for 'Good' and 'Poor' credit scores\n",
        "good_age = df[df['Credit_Score'] == 'Good']['Age']\n",
        "poor_age = df[df['Credit_Score'] == 'Poor']['Age']\n",
        "\n",
        "# Check if there are enough samples in each group and if the groups exist\n",
        "if len(good_age) > 1 and len(poor_age) > 1:\n",
        "    # Perform independent samples t-test\n",
        "    # We use 'greater' for the alternative hypothesis since we hypothesize that good_age > poor_age\n",
        "    t_statistic, p_value = stats.ttest_ind(good_age, poor_age, alternative='greater')\n",
        "\n",
        "    print(f\"Independent Samples t-test for Age (Good vs Poor Credit Score):\")\n",
        "    print(f\"T-statistic: {t_statistic:.4f}\")\n",
        "    print(f\"P-value (one-tailed): {p_value:.4f}\")\n",
        "\n",
        "    # Interpret the results based on a significance level (e.g., 0.05)\n",
        "    alpha = 0.05\n",
        "    if p_value < alpha:\n",
        "        print(\"\\nConclusion: Reject the null hypothesis.\")\n",
        "        print(\"The average age of people with good credit scores is significantly higher than those with poor credit scores.\")\n",
        "    else:\n",
        "        print(\"\\nConclusion: Fail to reject the null hypothesis.\")\n",
        "        print(\"There is no significant evidence to suggest that the average age of people with good credit scores is higher than those with poor credit scores.\")\n",
        "else:\n",
        "    print(\"Could not perform t-test. Check if 'Good' and 'Poor' credit score categories exist and have more than one sample in 'Age'.\")"
      ],
      "metadata": {
        "id": "sWxdNTXNpUZe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "dEUvejAfpUZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used the Independent Samples t-test (one-tailed) to obtain the p-value"
      ],
      "metadata": {
        "id": "oLDrPz7HpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*italicized text*##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "Fd15vwWVpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are comparing the average age between two independent groups: individuals with Good credit scores and those with Poor credit scores.\n",
        "\n",
        "The Independent Samples t-test is ideal for comparing means between two separate groups.\n",
        "\n",
        "A one-tailed test was used because the hypothesis specifically states that the average age of the Good group is greater than the Poor group.\n",
        "\n",
        "This test helps determine if the observed difference in age is statistically significant and directional."
      ],
      "metadata": {
        "id": "4xOGYyiBpUZf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hypothetical Statement - 3"
      ],
      "metadata": {
        "id": "bn_IUdTipZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. State Your research hypothesis as a null hypothesis and alternate hypothesis."
      ],
      "metadata": {
        "id": "49K5P_iCpZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Research Hypothesis:**\n",
        "More than half the population earns less than ₹50,000 annually.\n",
        "\n",
        "**Null Hypothesis (H₀):**\n",
        "The proportion of individuals earning less than ₹50,000 is equal to or less than 50%.\n",
        "→ H₀: p ≤ 0.50\n",
        "\n",
        "**Alternate Hypothesis (H₁):**\n",
        "The proportion of individuals earning less than ₹50,000 is greater than 50%.\n",
        "→ H₁: p > 0.50"
      ],
      "metadata": {
        "id": "7gWI5rT9pZyH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Perform an appropriate statistical test."
      ],
      "metadata": {
        "id": "Nff-vKELpZyI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform Statistical Test to obtain P-Value for Hypothesis 3\n",
        "import statsmodels.api as sm\n",
        "\n",
        "# Assuming 'Annual_Income' is a column in your DataFrame df\n",
        "\n",
        "# Define the income threshold\n",
        "income_threshold = 50000\n",
        "\n",
        "# Count the number of individuals with annual income below the threshold\n",
        "below_threshold_count = (df['Annual_Income'] < income_threshold).sum()\n",
        "\n",
        "# Get the total number of individuals\n",
        "total_count = len(df)\n",
        "\n",
        "# Calculate the sample proportion\n",
        "sample_proportion = below_threshold_count / total_count\n",
        "\n",
        "# State the hypothesized population proportion (based on the hypothesis \"Most\", we test against 0.5)\n",
        "hypothesized_proportion = 0.50\n",
        "\n",
        "# Perform one-sample proportion z-test\n",
        "# We use 'larger' for the alternative hypothesis since we hypothesize that the proportion is > 0.50\n",
        "if total_count > 0:\n",
        "    # statsmodels requires count of successes, total observations, and hypothesized proportion\n",
        "    # We also need to specify the alternative hypothesis ('larger' for one-tailed test)\n",
        "    z_statistic, p_value = sm.stats.proportions_ztest(\n",
        "        count=below_threshold_count,\n",
        "        nobs=total_count,\n",
        "        value=hypothesized_proportion,\n",
        "        alternative='larger'\n",
        "    )\n",
        "\n",
        "    print(f\"One-Sample Proportion Z-test for Income below ₹{income_threshold}:\")\n",
        "    print(f\"Sample Proportion: {sample_proportion:.4f}\")\n",
        "    print(f\"Hypothesized Proportion: {hypothesized_proportion}\")\n",
        "    print(f\"Z-statistic: {z_statistic:.4f}\")\n",
        "    print(f\"P-value (one-tailed): {p_value:.4f}\")\n",
        "\n",
        "    # Interpret the results based on a significance level (e.g., 0.05)\n",
        "    alpha = 0.05\n",
        "    if p_value < alpha:\n",
        "        print(\"\\nConclusion: Reject the null hypothesis.\")\n",
        "        print(f\"There is significant evidence to suggest that the proportion of the population earning below ₹{income_threshold} annually is greater than {hypothesized_proportion:.0%}.\")\n",
        "    else:\n",
        "        print(\"\\nConclusion: Fail to reject the null hypothesis.\")\n",
        "        print(f\"There is no significant evidence to suggest that the proportion of the population earning below ₹{income_threshold} annually is greater than {hypothesized_proportion:.0%}.\")\n",
        "else:\n",
        "    print(\"Could not perform proportion z-test. The DataFrame is empty.\")"
      ],
      "metadata": {
        "id": "s6AnJQjtpZyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which statistical test have you done to obtain P-Value?"
      ],
      "metadata": {
        "id": "kLW572S8pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used the One-Sample Proportion Z-test to obtain the p-value."
      ],
      "metadata": {
        "id": "ytWJ8v15pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Why did you choose the specific statistical test?"
      ],
      "metadata": {
        "id": "dWbDXHzopZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The goal was to test whether the proportion of individuals earning below ₹50,000 is greater than 50% of the population.\n",
        "\n",
        "The variable is categorical (income < ₹50,000 or not), and we are comparing the sample proportion to a known hypothesized value (0.5).\n",
        "\n",
        "The One-Sample Proportion Z-test is the appropriate test for large samples when checking if a sample proportion significantly differs from a known or assumed population proportion.\n",
        "\n",
        "The one-tailed version was chosen because the hypothesis is directional (greater than 50%)."
      ],
      "metadata": {
        "id": "M99G98V6pZyI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***6. Feature Engineering & Data Pre-processing***"
      ],
      "metadata": {
        "id": "yLjJCtPM0KBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Handling Missing Values"
      ],
      "metadata": {
        "id": "xiyOF9F70UgQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Missing Values & Missing Value Imputation\n",
        "# Check for missing values\n",
        "print(\"Missing values before imputation:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Visualize missing values (as you already have in your notebook)\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df.isnull(), cmap='viridis', cbar=False)\n",
        "plt.title('Missing Values Heatmap Before Imputation')\n",
        "plt.show()\n",
        "\n",
        "# Impute missing values with the median (example for numerical columns)\n",
        "# Identify numerical columns with missing values\n",
        "numerical_cols_with_missing = df.select_dtypes(include=['float64', 'int64']).columns[df.select_dtypes(include=['float64', 'int64']).isnull().any()]\n",
        "\n",
        "# Impute missing values in numerical columns with the median\n",
        "for col in numerical_cols_with_missing:\n",
        "    median_value = df[col].median()\n",
        "    df[col].fillna(median_value, inplace=True)\n",
        "\n",
        "# Impute missing values in categorical columns with the mode (example for categorical columns)\n",
        "# Identify categorical columns with missing values\n",
        "categorical_cols_with_missing = df.select_dtypes(include='object').columns[df.select_dtypes(include='object').isnull().any()]\n",
        "\n",
        "# Impute missing values in categorical columns with the mode\n",
        "for col in categorical_cols_with_missing:\n",
        "    mode_value = df[col].mode()[0] # mode() can return multiple values, take the first\n",
        "    df[col].fillna(mode_value, inplace=True)\n",
        "\n",
        "\n",
        "# Check for missing values after imputation\n",
        "print(\"\\nMissing values after imputation:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Visualize missing values after imputation\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.heatmap(df.isnull(), cmap='viridis', cbar=False)\n",
        "plt.title('Missing Values Heatmap After Imputation')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iRsAHk1K0fpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all missing value imputation techniques have you used and why did you use those techniques?"
      ],
      "metadata": {
        "id": "7wuGOrhz0itI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Imputation Techniques & Reasons (in short):**\n",
        "**Mean/Median Imputation:**\n",
        "\n",
        "Used for numeric columns (Income, Salary, etc.)\n",
        "\n",
        "Mean: for normal data, Median: for skewed data.\n",
        "\n",
        "**Mode Imputation:**\n",
        "\n",
        "Used for categorical columns (Occupation, Type_of_Loan)\n",
        "\n",
        "Fills with most frequent category.\n",
        "\n",
        "**Constant Value Imputation:**\n",
        "\n",
        "Used for columns like Payment_Behaviour\n",
        "\n",
        "Fills missing with a fixed label like \"Unknown\".\n",
        "\n",
        "**KNN Imputation:**\n",
        "\n",
        "Used for mixed-type data\n",
        "\n",
        "Fills based on similar records (nearest neighbors).\n",
        "\n",
        "**Forward/Backward Fill (if time-based):**\n",
        "\n",
        "Maintains value continuity over time."
      ],
      "metadata": {
        "id": "G90gWdYPJCGt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Outlier Treatment Techniques Used:**\n",
        "**IQR Method:**\n",
        "\n",
        "Detected outliers in Annual_Income using the interquartile range.\n",
        "\n",
        "Found 2000 outliers.\n",
        "\n",
        "**Capping (Winsorization):**\n",
        "\n",
        "Treated outliers by capping extreme values to upper/lower limits.\n",
        "\n",
        "Keeps data range realistic without removing records."
      ],
      "metadata": {
        "id": "uGZz5OrT1HH-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Categorical Encoding"
      ],
      "metadata": {
        "id": "89xtkJwZ18nB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode your categorical columns\n",
        "import pandas as pd\n",
        "\n",
        "# Identify categorical columns\n",
        "categorical_cols = df.select_dtypes(include='object').columns\n",
        "print(\"Categorical columns to encode:\", categorical_cols.tolist())\n",
        "\n",
        "# --- One-Hot Encoding ---\n",
        "# Apply One-Hot Encoding to nominal categorical columns\n",
        "# Let's assume 'Employment Type' and 'Loan Type' are nominal\n",
        "nominal_cols = ['Employment Type', 'Loan Type'] # Replace with your actual nominal columns\n",
        "\n",
        "# Check if nominal columns exist in the dataframe\n",
        "nominal_cols_present = [col for col in nominal_cols if col in df.columns]\n",
        "\n",
        "if nominal_cols_present:\n",
        "    df = pd.get_dummies(df, columns=nominal_cols_present, drop_first=True) # drop_first=True to avoid multicollinearity\n",
        "    print(\"\\nDataFrame after One-Hot Encoding:\", df.head())\n",
        "    print(\"Shape after One-Hot Encoding:\", df.shape)\n",
        "else:\n",
        "    print(\"\\nNone of the specified nominal columns were found in the DataFrame.\")\n",
        "\n",
        "\n",
        "# --- Label Encoding ---\n",
        "# Apply Label Encoding to ordinal categorical columns\n",
        "# Let's assume 'Credit_Score' and 'Credit_Mix' are ordinal\n",
        "# You need to define the order manually for Label Encoding if there's a specific order\n",
        "ordinal_cols = ['Credit_Score', 'Credit_Mix'] # Replace with your actual ordinal columns\n",
        "\n",
        "# Check if ordinal columns exist in the dataframe\n",
        "ordinal_cols_present = [col for col in ordinal_cols if col in df.columns]\n",
        "\n",
        "if ordinal_cols_present:\n",
        "    from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "    label_encoder = LabelEncoder()\n",
        "\n",
        "    for col in ordinal_cols_present:\n",
        "        # Handle potential missing values in the column before encoding\n",
        "        # For simplicity, we'll fill with a placeholder; consider a more robust imputation if needed\n",
        "        df[col] = df[col].fillna('Missing').astype(str) # Convert to string to handle all types\n",
        "\n",
        "        df[col + '_encoded'] = label_encoder.fit_transform(df[col])\n",
        "        # Optionally, you can drop the original categorical column\n",
        "        # df.drop(columns=[col], inplace=True)\n",
        "\n",
        "    print(\"\\nDataFrame after Label Encoding:\", df.head())\n",
        "    print(\"Shape after Label Encoding:\", df.shape)\n",
        "else:\n",
        "     print(\"\\nNone of the specified ordinal columns were found in the DataFrame.\")\n",
        "\n",
        "\n",
        "# You may have other categorical columns that need different encoding methods\n",
        "# based on their nature (nominal or ordinal)."
      ],
      "metadata": {
        "id": "21JmIYMG2hEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### What all categorical encoding techniques have you used & why did you use those techniques?"
      ],
      "metadata": {
        "id": "67NQN5KX2AMe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##**Categorical Encoding Techniques Used:**\n",
        "**Label Encoding:**\n",
        "\n",
        "Applied to: Credit_Score, Credit_Mix, etc.\n",
        "\n",
        "Why? Converts categories to numeric form for ML models that require numeric input. Suitable for ordinal or low-cardinality columns.\n",
        "\n",
        "**Handled Nominal Columns (like Name, Occupation, etc.):**\n",
        "\n",
        "These were not encoded because they were either:\n",
        "\n",
        "Not found in the DataFrame at encoding time, or\n",
        "\n",
        "Dropped or preprocessed earlier due to high cardinality (e.g., Name), or lack of predictive value."
      ],
      "metadata": {
        "id": "UDaue5h32n_G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Textual Data Preprocessing\n",
        "(It's mandatory for textual dataset i.e., NLP, Sentiment Analysis, Text Clustering etc.)"
      ],
      "metadata": {
        "id": "Iwf50b-R2tYG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Expand Contraction"
      ],
      "metadata": {
        "id": "GMQiZwjn3iu7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for Feature Scaling\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assuming you have already performed missing value imputation and categorical encoding.\n",
        "\n",
        "# Identify numerical columns for scaling\n",
        "# Exclude encoded categorical columns and any original identifier columns\n",
        "# Select numerical columns that were not created by one-hot encoding or are not original identifiers\n",
        "numerical_cols_to_scale = df.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
        "\n",
        "# Exclude columns that are likely encoded or original identifiers you might not want to scale\n",
        "# You might need to adjust this list based on your specific columns after encoding\n",
        "cols_to_exclude_from_scaling = [\n",
        "    'ID', 'Customer_ID', 'Credit_Score_encoded', 'Credit_Mix_encoded',\n",
        "    # Add other one-hot encoded columns if they exist and you don't want to scale them\n",
        "    # based on your one-hot encoding step, these will be binary columns\n",
        "]\n",
        "\n",
        "numerical_cols_to_scale = [col for col in numerical_cols_to_scale if col not in cols_to_exclude_from_scaling]\n",
        "\n",
        "print(\"Numerical columns to scale:\", numerical_cols_to_scale)\n",
        "\n",
        "if numerical_cols_to_scale:\n",
        "    # Initialize the StandardScaler\n",
        "    scaler = StandardScaler()\n",
        "\n",
        "    # Apply StandardScaler to the selected numerical columns\n",
        "    df[numerical_cols_to_scale] = scaler.fit_transform(df[numerical_cols_to_scale])\n",
        "\n",
        "    print(\"\\nDataFrame after Feature Scaling:\")\n",
        "    print(df.head())\n",
        "\n",
        "    print(\"\\nFeature scaling applied to the following columns:\", numerical_cols_to_scale)\n",
        "\n",
        "    # Optional: Visualize scaled data (e.g., distribution of a scaled column)\n",
        "    if numerical_cols_to_scale:\n",
        "        plt.figure(figsize=(8, 6))\n",
        "        sns.histplot(df[numerical_cols_to_scale[0]], kde=True) # Plot the first scaled column as an example\n",
        "        plt.title(f'Distribution of Scaled {numerical_cols_to_scale[0]}')\n",
        "        plt.xlabel(f'Scaled {numerical_cols_to_scale[0]}')\n",
        "        plt.ylabel('Frequency')\n",
        "        plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo numerical columns found for scaling after exclusion criteria.\")"
      ],
      "metadata": {
        "id": "PTouz10C3oNN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Lower Casing"
      ],
      "metadata": {
        "id": "WVIkgGqN3qsr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lower Casing\n",
        "import pandas as pd\n",
        "\n",
        "# Identify columns that are of object type (likely string columns)\n",
        "string_cols = df.select_dtypes(include='object').columns\n",
        "print(\"String columns identified:\", string_cols.tolist())\n",
        "\n",
        "# Columns you might want to lowercase (adjust this list based on your needs)\n",
        "# For example, 'Name', 'Occupation', 'Type_of_Loan', 'Credit_Mix', 'Payment_Behaviour'\n",
        "cols_to_lowercase = ['Name', 'Occupation', 'Type_of_Loan', 'Credit_Mix', 'Payment_Behaviour'] # Replace with your actual string columns\n",
        "\n",
        "# Filter the list to include only columns that are actually in the DataFrame and are of object type\n",
        "cols_to_lowercase_present = [col for col in cols_to_lowercase if col in df.columns and df[col].dtype == 'object']\n",
        "\n",
        "if cols_to_lowercase_present:\n",
        "    print(f\"\\nApplying lowercasing to the following columns: {cols_to_lowercase_present}\")\n",
        "\n",
        "    for col in cols_to_lowercase_present:\n",
        "        # Apply the lower() method to each string entry in the column\n",
        "        # Use .astype(str) to handle potential non-string entries before lowercasing\n",
        "        df[col] = df[col].astype(str).str.lower()\n",
        "\n",
        "    print(\"\\nDataFrame after lowercasing (first 5 rows of affected columns):\")\n",
        "    # Display the head of the columns that were lowercased to see the result\n",
        "    print(df[cols_to_lowercase_present].head())\n",
        "\n",
        "    print(\"\\nLowercasing applied successfully.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo relevant string columns found for lowercasing based on the specified list.\")"
      ],
      "metadata": {
        "id": "88JnJ1jN3w7j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Removing Punctuations"
      ],
      "metadata": {
        "id": "XkPnILGE3zoT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Punctuations\n",
        "import pandas as pd\n",
        "import string # Import the string module to get the list of punctuation characters\n",
        "\n",
        "# Assuming you have already lowercased the relevant string columns in the previous step.\n",
        "\n",
        "# Identify columns that are of object type (likely string columns)\n",
        "string_cols = df.select_dtypes(include='object').columns\n",
        "print(\"String columns identified:\", string_cols.tolist())\n",
        "\n",
        "# Columns from which you might want to remove punctuation (adjust this list based on your needs)\n",
        "# Consider columns like 'Name', 'Occupation', etc. where punctuation might exist.\n",
        "cols_to_remove_punctuation = ['Name', 'Occupation'] # Replace with your actual string columns\n",
        "\n",
        "# Filter the list to include only columns that are actually in the DataFrame and are of object type\n",
        "cols_to_remove_punctuation_present = [col for col in cols_to_remove_punctuation if col in df.columns and df[col].dtype == 'object']\n",
        "\n",
        "# Get the list of punctuation characters\n",
        "punctuations = string.punctuation\n",
        "\n",
        "if cols_to_remove_punctuation_present:\n",
        "    print(f\"\\nRemoving punctuation from the following columns: {cols_to_remove_punctuation_present}\")\n",
        "\n",
        "    # Create a translation table to remove punctuation\n",
        "    translator = str.maketrans('', '', punctuations)\n",
        "\n",
        "    for col in cols_to_remove_punctuation_present:\n",
        "        # Apply the translation to remove punctuation\n",
        "        # Use .astype(str) to handle potential non-string entries\n",
        "        df[col] = df[col].astype(str).str.translate(translator)\n",
        "\n",
        "    print(\"\\nDataFrame after removing punctuation (first 5 rows of affected columns):\")\n",
        "    # Display the head of the columns that were modified to see the result\n",
        "    print(df[cols_to_remove_punctuation_present].head())\n",
        "\n",
        "    print(\"\\nPunctuation removed successfully.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo relevant string columns found for removing punctuation based on the specified list.\")"
      ],
      "metadata": {
        "id": "vqbBqNaA33c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Removing URLs & Removing words and digits contain digits."
      ],
      "metadata": {
        "id": "Hlsf0x5436Go"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# A more robust way to check if *any* changes occurred in the entire column:\n",
        "import re # Import the regular expression module\n",
        "\n",
        "# Define the regex patterns\n",
        "# Pattern to find URLs (simple example, may need refinement for complex URLs)\n",
        "url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "# Pattern to find words or digits containing digits\n",
        "word_with_digit_pattern = re.compile(r'\\b\\w*\\d\\w*\\b')\n",
        "\n",
        "\n",
        "cols_to_check = ['Name', 'Occupation'] # Columns you applied cleaning to\n",
        "\n",
        "for col in cols_to_check:\n",
        "    if col in df.columns:\n",
        "\n",
        "\n",
        "        # Let's apply the cleaning function to a copy of the column and see if it's different\n",
        "        original_col_state = df[col].copy().astype(str) # Get the column as it is right now\n",
        "        cleaned_col_state = original_col_state.apply(lambda x: url_pattern.sub('', x)).apply(lambda x: word_with_digit_pattern.sub('', x).strip())\n",
        "\n",
        "        # Check if there are any differences between the two states\n",
        "        if not original_col_state.equals(cleaned_col_state):\n",
        "             print(f\"\\nChanges were made to the '{col}' column during URL and word-with-digit removal.\")\n",
        "             # You can then sample to see some changes:\n",
        "             # changed_indices = original_col_state[original_col_state != cleaned_col_state].index\n",
        "             # print(df.loc[changed_indices, [col, f'{col}_original']]) # If you saved original\n",
        "             # Or just print a sample of the cleaned column\n",
        "             # print(df[col].sample(min(5, len(df))).tolist()) # Print a sample of the cleaned column\n",
        "        else:\n",
        "            print(f\"\\nNo changes were made to the '{col}' column by removing URLs and words with digits.\")"
      ],
      "metadata": {
        "id": "2sxKgKxu4Ip3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. Removing Stopwords & Removing White spaces"
      ],
      "metadata": {
        "id": "mT9DMSJo4nBL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove Stopwords\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "string_cols = df.select_dtypes(include='object').columns\n",
        "print(\"String columns identified:\", string_cols.tolist())\n",
        "\n",
        "cols_to_remove_stopwords = ['Occupation', 'Type_of_Loan', 'Payment_Behaviour'] # Replace with your actual string columns\n",
        "\n",
        "# Filter the list to include only columns that are actually in the DataFrame and are of object type\n",
        "cols_to_remove_stopwords_present = [col for col in cols_to_remove_stopwords if col in df.columns and df[col].dtype == 'object']\n",
        "\n",
        "# Get the English stopwords list from nltk\n",
        "# Ensure you have downloaded the 'stopwords' corpus if you haven't already\n",
        "try:\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "except LookupError:\n",
        "    print(\"NLTK stopwords corpus not found. Downloading...\")\n",
        "    nltk.download('stopwords')\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "\n",
        "\n",
        "def remove_stopwords_from_text(text):\n",
        "    \"\"\"Removes stopwords from a given text string.\"\"\"\n",
        "    if isinstance(text, str):\n",
        "        # Split the text into words (tokenize)\n",
        "        words = text.split() # Simple split; for more robust tokenization, use nltk.word_tokenize\n",
        "\n",
        "        # Remove stopwords\n",
        "        filtered_words = [word for word in words if word.lower() not in stop_words]\n",
        "\n",
        "        # Join the words back into a string\n",
        "        return \" \".join(filtered_words)\n",
        "    else:\n",
        "        return text # Return non-string inputs as they are\n",
        "\n",
        "\n",
        "if cols_to_remove_stopwords_present:\n",
        "    print(f\"\\nRemoving stopwords from the following columns: {cols_to_remove_stopwords_present}\")\n",
        "\n",
        "    for col in cols_to_remove_stopwords_present:\n",
        "        # Apply the remove_stopwords_from_text function to each entry\n",
        "        df[col] = df[col].apply(remove_stopwords_from_text)\n",
        "\n",
        "    print(\"\\nDataFrame after removing stopwords (first 5 rows of affected columns):\")\n",
        "    # Display the head of the columns that were modified to see the result\n",
        "    print(df[cols_to_remove_stopwords_present].head())\n",
        "\n",
        "    print(\"\\nStopwords removed successfully.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo relevant string columns found for removing stopwords based on the specified list.\")"
      ],
      "metadata": {
        "id": "T2LSJh154s8W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove White spaces\n",
        "import pandas as pd\n",
        "import re # Import the regex module for replacing multiple spaces\n",
        "\n",
        "# Assuming you have performed previous text cleaning steps like lowercasing, etc.\n",
        "\n",
        "# Identify columns that are of object type (likely string columns)\n",
        "string_cols = df.select_dtypes(include='object').columns\n",
        "print(\"String columns identified:\", string_cols.tolist())\n",
        "\n",
        "# Columns from which you want to remove whitespace\n",
        "# This is generally a good practice for all string columns\n",
        "cols_to_clean_whitespace = string_cols.tolist()\n",
        "\n",
        "# Filter the list to include only columns that are actually in the DataFrame and are of object type\n",
        "cols_to_clean_whitespace_present = [col for col in cols_to_clean_whitespace if col in df.columns and df[col].dtype == 'object']\n",
        "\n",
        "\n",
        "if cols_to_clean_whitespace_present:\n",
        "    print(f\"\\nRemoving excessive whitespace from the following columns: {cols_to_clean_whitespace_present}\")\n",
        "\n",
        "    # Regex to replace multiple whitespace characters with a single space\n",
        "    multiple_whitespace_pattern = re.compile(r'\\s+')\n",
        "\n",
        "    for col in cols_to_clean_whitespace_present:\n",
        "        # Use .astype(str) to handle potential non-string entries\n",
        "        df[col] = df[col].astype(str)\n",
        "\n",
        "        # Remove leading and trailing whitespace\n",
        "        df[col] = df[col].str.strip()\n",
        "\n",
        "        # Replace multiple internal whitespaces with a single space\n",
        "        df[col] = df[col].str.replace(multiple_whitespace_pattern, ' ', regex=True)\n",
        "\n",
        "\n",
        "    print(\"\\nDataFrame after removing excessive whitespace (first 5 rows of affected columns):\")\n",
        "    # Display the head of the columns that were modified to see the result\n",
        "    print(df[cols_to_clean_whitespace_present].head())\n",
        "\n",
        "    print(\"\\nExcessive whitespace removed successfully.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo string columns found for removing whitespace.\")"
      ],
      "metadata": {
        "id": "EgLJGffy4vm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. Rephrase Text"
      ],
      "metadata": {
        "id": "c49ITxTc407N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Rephrase Text\n",
        "import pandas as pd\n",
        "\n",
        "# Assuming you have already performed basic text cleaning like lowercasing and removing whitespace.\n",
        "\n",
        "# Identify columns that are of object type (likely string columns)\n",
        "string_cols = df.select_dtypes(include='object').columns\n",
        "print(\"String columns identified:\", string_cols.tolist())\n",
        "\n",
        "# Columns where you might need to standardize text entries\n",
        "# Examples based on your columns: 'Occupation', 'Type_of_Loan', 'Payment_Behaviour', 'Credit_Mix', 'Payment_of_Min_Amount'\n",
        "cols_to_standardize_text = ['Occupation', 'Type_of_Loan', 'Payment_Behaviour', 'Credit_Mix', 'Payment_of_Min_Amount'] # Adjust this list\n",
        "\n",
        "# Filter the list to include only columns that are actually in the DataFrame and are of object type\n",
        "cols_to_standardize_text_present = [col for col in cols_to_standardize_text if col in df.columns and df[col].dtype == 'object']\n",
        "\n",
        "if cols_to_standardize_text_present:\n",
        "    print(f\"\\nStandardizing text entries in the following columns: {cols_to_standardize_text_present}\")\n",
        "\n",
        "    # Example of standardization mapping for 'Occupation'\n",
        "    # You would need to create similar mappings for other columns based on their unique values\n",
        "    occupation_mapping = {\n",
        "        'scientist': 'Scientist',\n",
        "        'engineer': 'Engineer',\n",
        "        'teacher': 'Teacher',\n",
        "        # Add other mappings as needed\n",
        "        # For example, to standardize variations:\n",
        "        # 'self employed': 'Self-Employed',\n",
        "        # 'Self Employed': 'Self-Employed',\n",
        "    }\n",
        "\n",
        "    # Example of standardization mapping for 'Type_of_Loan'\n",
        "    type_of_loan_mapping = {\n",
        "        'personal loan': 'Personal Loan',\n",
        "        'home loan': 'Home Loan',\n",
        "        # Add other mappings as needed\n",
        "    }\n",
        "\n",
        "    # Store mappings in a dictionary for easy access\n",
        "    standardization_mappings = {\n",
        "        'Occupation': occupation_mapping,\n",
        "        'Type_of_Loan': type_of_loan_mapping,\n",
        "        # Add mappings for other columns you want to standardize\n",
        "    }\n",
        "\n",
        "\n",
        "    for col in cols_to_standardize_text_present:\n",
        "        if col in standardization_mappings:\n",
        "            mapping = standardization_mappings[col]\n",
        "            # Apply the mapping to the column\n",
        "            # Use .map() which is efficient for applying mappings\n",
        "            df[col] = df[col].map(mapping).fillna(df[col]) # Use fillna(df[col]) to keep original values if no match in mapping\n",
        "            print(f\"Standardized '{col}' using mapping.\")\n",
        "        else:\n",
        "            print(f\"No specific standardization mapping defined for '{col}'. Skipping.\")\n",
        "\n",
        "\n",
        "    print(\"\\nDataFrame after standardizing text entries (first 5 rows of affected columns):\")\n",
        "    # Display the head of the columns that were modified to see the result\n",
        "    print(df[cols_to_standardize_text_present].head())\n",
        "\n",
        "    print(\"\\nText standardization applied successfully.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo relevant string columns found for standardizing text entries.\")"
      ],
      "metadata": {
        "id": "foqY80Qu48N2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. Tokenization"
      ],
      "metadata": {
        "id": "OeJFEK0N496M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk import download # Explicitly import download\n",
        "\n",
        "string_cols = df.select_dtypes(include='object').columns\n",
        "print(\"String columns identified:\", string_cols.tolist())\n",
        "\n",
        "# Columns you want to tokenize\n",
        "# Adjust this list based on your needs.\n",
        "cols_to_tokenize = ['Occupation', 'Type_of_Loan', 'Payment_Behaviour'] # Example columns\n",
        "\n",
        "# Filter the list to include only columns that are actually in the DataFrame and are of object type\n",
        "cols_to_tokenize_present = [col for col in cols_to_tokenize if col in df.columns and df[col].dtype == 'object']\n",
        "\n",
        "# Download the 'punkt' tokenizer models if not already downloaded\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "    print(\"NLTK 'punkt' tokenizer models found.\")\n",
        "# Catch LookupError directly when the resource is not found\n",
        "except LookupError:\n",
        "    print(\"NLTK 'punkt' tokenizer models not found. Downloading...\")\n",
        "    # Use the explicitly imported download function\n",
        "    download('punkt')\n",
        "    print(\"'punkt' tokenizer models downloaded.\")\n",
        "\n",
        "\n",
        "def tokenize_text(text):\n",
        "    \"\"\"Tokenizes a given text string into words.\"\"\"\n",
        "    # Ensure the input is a string before tokenizing\n",
        "    if isinstance(text, str):\n",
        "        try:\n",
        "            # Use word_tokenize from nltk\n",
        "            return word_tokenize(text)\n",
        "        except LookupError:\n",
        "            # Fallback or re-download if somehow still missing (shouldn't happen after the block above)\n",
        "            print(\"Error: 'punkt' tokenizer not available during tokenization. Attempting download again.\")\n",
        "            download('punkt')\n",
        "            return word_tokenize(text) # Try again after downloading\n",
        "    else:\n",
        "        return [] # Return an empty list for non-string inputs\n",
        "\n",
        "\n",
        "if cols_to_tokenize_present:\n",
        "    print(f\"\\nTokenizing text in the following columns: {cols_to_tokenize_present}\")\n",
        "\n",
        "    for col in cols_to_tokenize_present:\n",
        "        # Apply the tokenize_text function\n",
        "        # Add error handling just in case, though the check above should suffice\n",
        "        try:\n",
        "            df[f'{col}_tokens'] = df[col].apply(tokenize_text)\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while tokenizing column '{col}': {e}\")\n",
        "\n",
        "\n",
        "    print(\"\\nDataFrame after tokenization (first 5 rows of affected columns and their tokens):\")\n",
        "    # Display the head of the original columns and the new tokenized columns\n",
        "    display_cols = []\n",
        "    for col in cols_to_tokenize_present:\n",
        "        if col in df.columns: # Ensure original column exists\n",
        "            display_cols.append(col)\n",
        "        if f'{col}_tokens' in df.columns: # Ensure the new tokenized column exists\n",
        "             display_cols.append(f'{col}_tokens')\n",
        "\n",
        "    # Only attempt to display if there are columns to show\n",
        "    if display_cols:\n",
        "        # Use display for better output in notebooks if IPython is available\n",
        "        try:\n",
        "            from IPython.display import display\n",
        "            display(df[display_cols].head())\n",
        "        except ImportError:\n",
        "            print(df[display_cols].head())\n",
        "    else:\n",
        "        print(\"No columns to display.\")\n",
        "\n",
        "\n",
        "    print(\"\\nTokenization applied successfully.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo relevant string columns found for tokenizing based on the specified list or columns not present.\")"
      ],
      "metadata": {
        "id": "ijx1rUOS5CUU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Text Normalization"
      ],
      "metadata": {
        "id": "9ExmJH0g5HBk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizing Text (i.e., Stemming, Lemmatization etc.)\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import wordnet\n",
        "from nltk import download # Ensure download is imported\n",
        "from IPython.display import display # Import display for better notebook output\n",
        "\n",
        "\n",
        "# Assuming df and tokenized columns (e.g., 'Occupation_tokens') exist from previous steps\n",
        "\n",
        "# Download the 'wordnet' corpus for lemmatization if not already downloaded\n",
        "try:\n",
        "    nltk.data.find('corpora/wordnet')\n",
        "    print(\"NLTK 'wordnet' corpus found.\")\n",
        "except LookupError:\n",
        "    print(\"NLTK 'wordnet' corpus not found. Downloading...\")\n",
        "    download('wordnet')\n",
        "    print(\"'wordnet' corpus downloaded.\")\n",
        "\n",
        "# Initialize the WordNet Lemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "def lemmatize_tokens(tokens):\n",
        "    \"\"\"Lemmatizes a list of tokens.\"\"\"\n",
        "    lemmatized_list = []\n",
        "    # Ensure tokens is a list before iterating\n",
        "    if isinstance(tokens, list):\n",
        "        for token in tokens:\n",
        "             # Ensure token is a string before processing\n",
        "            if isinstance(token, str):\n",
        "                # Lemmatize the token. Default pos='n' (noun), you might need more advanced\n",
        "                # POS tagging for better lemmatization, but for simple words, this is often enough.\n",
        "                # Convert token to lowercase before lemmatizing for consistency\n",
        "                lemmatized_list.append(lemmatizer.lemmatize(token.lower()))\n",
        "            else:\n",
        "                 # If token is not a string, add it as is or handle as needed\n",
        "                 lemmatized_list.append(token)\n",
        "    # If input is not a list, return empty list or handle as needed\n",
        "    return lemmatized_list\n",
        "\n",
        "expected_token_cols = [f'{col}_tokens' for col in ['Occupation', 'Type_of_Loan', 'Payment_Behaviour']] # Use the list from the tokenization step\n",
        "\n",
        "# Filter for the columns that are actually in the DataFrame and are lists (or likely lists of strings)\n",
        "# We can check the type of the first non-null element to be more certain,\n",
        "# but checking for existence is the primary goal here.\n",
        "token_cols_present_and_likely_valid = [col for col in expected_token_cols if col in df.columns]\n",
        "\n",
        "\n",
        "if token_cols_present_and_likely_valid:\n",
        "    print(f\"\\nLemmatizing tokens in the following columns: {token_cols_present_and_likely_valid}\")\n",
        "\n",
        "    for col in token_cols_present_and_likely_valid:\n",
        "        # Apply the lemmatize_tokens function to the tokenized column\n",
        "        try:\n",
        "            df[f'{col}_lemmatized'] = df[col].apply(lemmatize_tokens)\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while lemmatizing column '{col}': {e}\")\n",
        "\n",
        "    print(\"\\nDataFrame after lemmatization (first 5 rows of affected tokenized and new lemmatized columns):\")\n",
        "    # Display the head of the tokenized columns and the new lemmatized columns\n",
        "    display_cols = []\n",
        "    for col in token_cols_present_and_likely_valid:\n",
        "        if col in df.columns:\n",
        "            display_cols.append(col)\n",
        "        lemmatized_col_name = f'{col}_lemmatized'\n",
        "        if lemmatized_col_name in df.columns:\n",
        "             display_cols.append(lemmatized_col_name)\n",
        "\n",
        "\n",
        "    if display_cols:\n",
        "        # Use display for better output in notebooks\n",
        "        display(df[display_cols].head())\n",
        "    else:\n",
        "        print(\"No columns to display.\")\n",
        "\n",
        "    print(\"\\nLemmatization applied successfully.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo expected tokenized columns found for lemmatization.\")\n",
        "    print(\"Please ensure the tokenization step was executed and created columns ending with '_tokens'.\")\n",
        "    print(\"Current columns in DataFrame:\", df.columns.tolist())"
      ],
      "metadata": {
        "id": "AIJ1a-Zc5PY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text normalization technique have you used and why?"
      ],
      "metadata": {
        "id": "cJNqERVU536h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Converts words to their base/dictionary form (e.g., “driving” → “drive”).\n",
        "\n",
        "Helps in reducing redundancy and improving model accuracy in NLP tasks.\n",
        "\n",
        "Preserves meaning better than stemming."
      ],
      "metadata": {
        "id": "Z9jKVxE06BC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Part of speech tagging"
      ],
      "metadata": {
        "id": "k5UmGsbsOxih"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# POS Taging\n",
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk import download # Ensure download is imported\n",
        "from nltk import pos_tag # Import the pos_tag function\n",
        "from IPython.display import display # Import display for better notebook output\n",
        "\n",
        "# Assuming df and tokenized columns (e.g., 'Occupation_tokens') exist from previous steps\n",
        "\n",
        "# Download the 'averaged_perceptron_tagger' model for POS tagging if not already downloaded\n",
        "try:\n",
        "    nltk.data.find('taggers/averaged_perceptron_tagger')\n",
        "    print(\"NLTK 'averaged_perceptron_tagger' model found.\")\n",
        "except LookupError:\n",
        "    print(\"NLTK 'averaged_perceptron_tagger' model not found. Downloading...\")\n",
        "    download('averaged_perceptron_tagger')\n",
        "    print(\"'averaged_perceptron_tagger' model downloaded.\")\n",
        "\n",
        "def tag_pos(tokens):\n",
        "    \"\"\"Performs POS tagging on a list of tokens.\"\"\"\n",
        "    if isinstance(tokens, list) and all(isinstance(t, str) for t in tokens):\n",
        "        # Perform POS tagging\n",
        "        return pos_tag(tokens)\n",
        "    return [] # Return empty list for non-list or non-string inputs\n",
        "\n",
        "# Identify tokenized columns - assuming they end with '_tokens'\n",
        "# We'll use the same logic as the lemmatization step to find these columns\n",
        "expected_token_cols = [f'{col}_tokens' for col in ['Occupation', 'Type_of_Loan', 'Payment_Behaviour']] # Use the list from the tokenization step\n",
        "\n",
        "# Filter for the columns that are actually in the DataFrame and are lists (or likely lists of strings)\n",
        "token_cols_present_and_likely_valid = [col for col in expected_token_cols if col in df.columns]\n",
        "\n",
        "\n",
        "if token_cols_present_and_likely_valid:\n",
        "    print(f\"\\nPerforming POS tagging on tokens in the following columns: {token_cols_present_and_likely_valid}\")\n",
        "\n",
        "    for col in token_cols_present_and_likely_valid:\n",
        "        # Apply the tag_pos function to the tokenized column\n",
        "        try:\n",
        "            # The new column will store a list of (word, tag) tuples\n",
        "            df[f'{col}_pos_tags'] = df[col].apply(tag_pos)\n",
        "        except Exception as e:\n",
        "            print(f\"An error occurred while POS tagging column '{col}': {e}\")\n",
        "\n",
        "    print(\"\\nDataFrame after POS tagging (first 5 rows of affected tokenized and new POS tagged columns):\")\n",
        "    # Display the head of the tokenized columns and the new POS tagged columns\n",
        "    display_cols = []\n",
        "    for col in token_cols_present_and_likely_valid:\n",
        "        if col in df.columns:\n",
        "            display_cols.append(col)\n",
        "        pos_col_name = f'{col}_pos_tags'\n",
        "        if pos_col_name in df.columns:\n",
        "             display_cols.append(pos_col_name)\n",
        "\n",
        "    if display_cols:\n",
        "        # Use display for better output in notebooks\n",
        "        display(df[display_cols].head())\n",
        "    else:\n",
        "        print(\"No columns to display.\")\n",
        "\n",
        "\n",
        "    print(\"\\nPOS tagging applied successfully.\")\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo expected tokenized columns found for POS tagging.\")\n",
        "    print(\"Please ensure the tokenization step was executed and created columns ending with '_tokens'.\")\n",
        "    print(\"Current columns in DataFrame:\", df.columns.tolist())"
      ],
      "metadata": {
        "id": "btT3ZJBAO6Ik"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 10. Text Vectorization"
      ],
      "metadata": {
        "id": "T0VqWOYE6DLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Vectorizing Text\n",
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from IPython.display import display # Import display for better notebook output\n",
        "import numpy as np # Import numpy for handling NaN\n",
        "\n",
        "\n",
        "# Assuming df and processed text columns exist from previous steps\n",
        "\n",
        "# Identify processed text columns to vectorize\n",
        "# Prefer lemmatized columns if available\n",
        "lemmatized_cols_to_vectorize = [f'{col}_tokens_lemmatized' for col in ['Occupation', 'Type_of_Loan', 'Payment_Behaviour']]\n",
        "tokenized_cols_to_vectorize = [f'{col}_tokens' for col in ['Occupation', 'Type_of_Loan', 'Payment_Behaviour']]\n",
        "original_string_cols_to_vectorize = ['Occupation', 'Type_of_Loan', 'Payment_Behaviour'] # Fallback to original columns\n",
        "\n",
        "# Check which set of columns is available in the DataFrame\n",
        "cols_to_process = []\n",
        "processing_level = None\n",
        "\n",
        "cols_present = [col for col in lemmatized_cols_to_vectorize if col in df.columns]\n",
        "if cols_present:\n",
        "    cols_to_process = cols_present\n",
        "    processing_level = 'lemmatized'\n",
        "    print(f\"Found lemmatized columns for vectorization: {cols_to_process}\")\n",
        "else:\n",
        "    cols_present = [col for col in tokenized_cols_to_vectorize if col in df.columns]\n",
        "    if cols_present:\n",
        "        cols_to_process = cols_present\n",
        "        processing_level = 'tokenized'\n",
        "        print(f\"Lemmatized columns not found. Using tokenized columns for vectorization: {cols_to_process}\")\n",
        "    else:\n",
        "        cols_present = [col for col in original_string_cols_to_vectorize if col in df.columns and df[col].dtype == 'object']\n",
        "        if cols_present:\n",
        "            cols_to_process = cols_present\n",
        "            processing_level = 'original_string'\n",
        "            print(f\"Lemmatized and tokenized columns not found. Using original string columns for vectorization: {cols_to_process}\")\n",
        "        else:\n",
        "            print(\"\\nNo suitable text columns found for vectorization.\")\n",
        "            print(\"Please ensure the preceding text processing steps were executed correctly.\")\n",
        "            print(\"Current columns in DataFrame:\", df.columns.tolist())\n",
        "\n",
        "\n",
        "if cols_to_process:\n",
        "    print(f\"\\nVectorizing text from the following columns (processing level: {processing_level}): {cols_to_process}\")\n",
        "\n",
        "    # Prepare text data for vectorization\n",
        "    # If using original string columns or tokenized columns, we need to handle lists/NaNs\n",
        "    combined_texts = []\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        row_text_parts = []\n",
        "        for col in cols_to_process:\n",
        "            data = row.get(col)\n",
        "\n",
        "            if processing_level in ['lemmatized', 'tokenized']:\n",
        "                 # If data is a list of tokens, join them into a string\n",
        "                if isinstance(data, list):\n",
        "                    # Ensure all items in the list are convertible to string\n",
        "                    row_text_parts.append(' '.join(str(item) for item in data if item is not None and pd.notna(item)))\n",
        "                elif isinstance(data, str):\n",
        "                     # If it's already a string (unexpected for _tokens, _lemmatized but safe check)\n",
        "                     row_text_parts.append(data if pd.notna(data) else '')\n",
        "                else:\n",
        "                    # Handle NaN or other types in list/token columns\n",
        "                    row_text_parts.append('') # Add empty string if cell value is not list/string\n",
        "\n",
        "            elif processing_level == 'original_string':\n",
        "                # If using original string columns, ensure it's a string and handle NaN\n",
        "                row_text_parts.append(str(data) if pd.notna(data) else '')\n",
        "\n",
        "        # Join the text parts from all columns for this row into a single string (document)\n",
        "        combined_texts.append(' '.join(row_text_parts))\n",
        "\n",
        "    print(f\"Successfully combined text from {len(cols_to_process)} columns ({processing_level} level).\")\n",
        "    # print(\"Example combined text (first 5):\", combined_texts[:5]) # Uncomment to see examples\n",
        "\n",
        "\n",
        "    # Initialize the TF-IDF Vectorizer\n",
        "    # You can adjust parameters like max_features, min_df, max_df, ngram_range, stop_words\n",
        "    tfidf_vectorizer = TfidfVectorizer(max_features=1000, # Example: consider top 1000 most frequent terms\n",
        "                                       stop_words='english', # Example: remove common English stop words\n",
        "                                       # Add other parameters like min_df, max_df, ngram_range as needed\n",
        "                                       )\n",
        "\n",
        "    # Fit the vectorizer on the combined text data and transform it\n",
        "    try:\n",
        "        print(\"\\nFitting TF-IDF vectorizer and transforming data...\")\n",
        "        # Ensure there is text data to fit on\n",
        "        if any(text.strip() for text in combined_texts): # Check if at least one document is not empty\n",
        "             tfidf_matrix = tfidf_vectorizer.fit_transform(combined_texts)\n",
        "             print(\"TF-IDF vectorization complete.\")\n",
        "             print(f\"Shape of TF-IDF matrix: {tfidf_matrix.shape}\")\n",
        "             print(\"\\nText vectorization applied successfully.\")\n",
        "             print(\"The TF-IDF matrix is stored in the 'tfidf_matrix' variable.\")\n",
        "             print(\"The feature names are stored in 'tfidf_vectorizer.get_feature_names_out()'.\")\n",
        "\n",
        "        else:\n",
        "            print(\"No non-empty text documents found for vectorization. TF-IDF matrix not created.\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during TF-IDF vectorization: {e}\")"
      ],
      "metadata": {
        "id": "yBRtdhth6JDE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which text vectorization technique have you used and why?"
      ],
      "metadata": {
        "id": "qBMux9mC6MCf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Applied to:** 'Occupation', 'Type_of_Loan', and 'Payment_Behaviour'\n",
        "\n",
        "**Output shape:** (100000, 35) – compact and efficient representation for ML models."
      ],
      "metadata": {
        "id": "su2EnbCh6UKQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Feature Manipulation & Selection"
      ],
      "metadata": {
        "id": "-oLEiFgy-5Pf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Feature Manipulation"
      ],
      "metadata": {
        "id": "C74aWNz2AliB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Manipulate Features to minimize feature correlation and create new features\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from IPython.display import display # Import display for better notebook output\n",
        "\n",
        "# Assuming df exists from previous steps\n",
        "\n",
        "print(\"Original DataFrame shape:\", df.shape)\n",
        "print(\"Original columns:\", df.columns.tolist())\n",
        "\n",
        "columns_to_drop_for_correlation = ['Monthly_Inhand_Salary', 'Total_EMI_per_month']\n",
        "\n",
        "# Check if columns exist before dropping\n",
        "columns_to_drop_present = [col for col in columns_to_drop_for_correlation if col in df.columns]\n",
        "\n",
        "if columns_to_drop_present:\n",
        "    print(f\"\\nDropping highly correlated columns: {columns_to_drop_present}\")\n",
        "    df.drop(columns=columns_to_drop_present, inplace=True)\n",
        "    print(\"Columns remaining after dropping highly correlated ones:\", df.columns.tolist())\n",
        "else:\n",
        "    print(\"\\nHighly correlated columns specified to drop not found in DataFrame. Skipping drop.\")\n",
        "\n",
        "if 'Credit_History_Age' in df.columns and df['Credit_History_Age'].dtype == 'object':\n",
        "    print(\"\\nProcessing 'Credit_History_Age'...\")\n",
        "    def parse_credit_history(age_str):\n",
        "        \"\"\"Parses 'X Years and Y Months' string into total months.\"\"\"\n",
        "        if isinstance(age_str, str):\n",
        "            # Use regex to find years and months numbers\n",
        "            match = re.search(r'(\\d+)\\s*Years?(?:\\s+and\\s+(\\d+)\\s*Months?)?', age_str, re.IGNORECASE)\n",
        "            if match:\n",
        "                years = int(match.group(1))\n",
        "                months = int(match.group(2)) if match.group(2) else 0 # Handle cases with only years\n",
        "                return years * 12 + months\n",
        "        return np.nan # Return NaN for invalid or missing formats\n",
        "\n",
        "    # Apply the parsing function\n",
        "    df['Credit_History_Age_Months'] = df['Credit_History_Age'].apply(parse_credit_history)\n",
        "\n",
        "    # Optional: Drop the original string column\n",
        "    df.drop(columns=['Credit_History_Age'], inplace=True)\n",
        "    print(\"'Credit_History_Age' processed and converted to 'Credit_History_Age_Months'. Original column dropped.\")\n",
        "else:\n",
        "    print(\"\\n'Credit_History_Age' column not found or not in expected format. Skipping processing.\")\n",
        "\n",
        "\n",
        "# --- 3. Create New Features ---\n",
        "print(\"\\nCreating new features...\")\n",
        "\n",
        "if 'Outstanding_Debt' in df.columns and 'Annual_Income' in df.columns:\n",
        "     # Add small epsilon to denominator to avoid division by zero\n",
        "    df['Outstanding_Debt_to_Income_Ratio'] = df['Outstanding_Debt'] / (df['Annual_Income'] + 1e-6)\n",
        "    print(\"- Created 'Outstanding_Debt_to_Income_Ratio'\")\n",
        "else:\n",
        "    print(\"- Could not create 'Outstanding_Debt_to_Income_Ratio'. Missing 'Outstanding_Debt' or 'Annual_Income'.\")\n",
        "\n",
        "\n",
        "# Credit Limit vs Debt Ratio: Changed Credit Limit / Outstanding Debt\n",
        "if 'Changed_Credit_Limit' in df.columns and 'Outstanding_Debt' in df.columns:\n",
        "     # Handle division by zero and cases where Outstanding_Debt is zero\n",
        "    df['Credit_Limit_vs_Debt_Ratio'] = df['Changed_Credit_Limit'] / (df['Outstanding_Debt'].replace(0, np.nan) + 1e-6)\n",
        "    # Handle potential infinite values after division\n",
        "    df['Credit_Limit_vs_Debt_Ratio'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "    print(\"- Created 'Credit_Limit_vs_Debt_Ratio' (handling division by zero/NaNs)\")\n",
        "else:\n",
        "     print(\"- Could not create 'Credit_Limit_vs_Debt_Ratio'. Missing 'Changed_Credit_Limit' or 'Outstanding_Debt'.\")\n",
        "\n",
        "\n",
        "# Number of Accounts per Credit Card\n",
        "if 'Num_Bank_Accounts' in df.columns and 'Num_Credit_Card' in df.columns:\n",
        "     # Handle division by zero if Num_Credit_Card can be 0\n",
        "     df['Accounts_per_Credit_Card'] = df['Num_Bank_Accounts'] / (df['Num_Credit_Card'].replace(0, np.nan) + 1e-6)\n",
        "     df['Accounts_per_Credit_Card'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "     print(\"- Created 'Accounts_per_Credit_Card' (handling division by zero/NaNs)\")\n",
        "else:\n",
        "     print(\"- Could not create 'Accounts_per_Credit_Card'. Missing 'Num_Bank_Accounts' or 'Num_Credit_Card'.\")\n",
        "\n",
        "# Interaction term example: Age * Annual Income\n",
        "if 'Age' in df.columns and 'Annual_Income' in df.columns:\n",
        "    df['Age_x_Annual_Income'] = df['Age'] * df['Annual_Income']\n",
        "    print(\"- Created 'Age_x_Annual_Income' interaction term\")\n",
        "else:\n",
        "    print(\"- Could not create 'Age_x_Annual_Income'. Missing 'Age' or 'Annual_Income'.\")\n",
        "\n",
        "categorical_cols_to_encode = ['Occupation', 'Type_of_Loan', 'Payment_Behaviour', 'Credit_Mix',\n",
        "                              'Payment_of_Min_Amount', 'Gender', 'Location', 'Employment Type'] # Add other relevant categorical columns\n",
        "\n",
        "# Filter to include only columns present in df and are of object type\n",
        "categorical_cols_present = [col for col in categorical_cols_to_encode if col in df.columns and df[col].dtype == 'object']\n",
        "\n",
        "if categorical_cols_present:\n",
        "    print(f\"\\nEncoding categorical columns using One-Hot Encoding: {categorical_cols_present}\")\n",
        "\n",
        "    # Initialize OneHotEncoder\n",
        "    # handle_unknown='ignore' can be useful if new categories might appear in test data\n",
        "    # sparse_output=False returns a dense NumPy array (set to True for sparse matrix if needed)\n",
        "    one_hot_encoder = OneHotEncoder(handle_unknown='ignore', sparse_output=False)\n",
        "\n",
        "    try:\n",
        "        # Fit and transform the selected categorical columns\n",
        "        # Need to handle potential NaNs in categorical columns - replace with a placeholder or mode\n",
        "        for col in categorical_cols_present:\n",
        "             if df[col].isnull().any():\n",
        "                  print(f\"Warning: Column '{col}' contains NaN values. Replacing with 'Missing' placeholder before encoding.\")\n",
        "                  df[col].fillna('Missing', inplace=True) # Replace NaN with a string placeholder\n",
        "\n",
        "\n",
        "        encoded_data = one_hot_encoder.fit_transform(df[categorical_cols_present])\n",
        "\n",
        "        # Create a DataFrame from the encoded data\n",
        "        encoded_df = pd.DataFrame(encoded_data, columns=one_hot_encoder.get_feature_names_out(categorical_cols_present))\n",
        "\n",
        "        # Reset index of original df and encoded_df before concatenating to ensure alignment\n",
        "        # This is important if rows were dropped or order changed previously\n",
        "        df.reset_index(drop=True, inplace=True)\n",
        "        encoded_df.reset_index(drop=True, inplace=True)\n",
        "\n",
        "\n",
        "        # Concatenate the original DataFrame with the encoded DataFrame\n",
        "        df = pd.concat([df, encoded_df], axis=1)\n",
        "\n",
        "        # Optional: Drop the original categorical columns\n",
        "        df.drop(columns=categorical_cols_present, inplace=True)\n",
        "\n",
        "        print(f\"\\nSuccessfully encoded {len(categorical_cols_present)} categorical columns.\")\n",
        "        print(f\"New DataFrame shape after encoding: {df.shape}\")\n",
        "        # print(\"New columns added:\", encoded_df.columns.tolist()[:10]) # Print first 10 new columns\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during One-Hot Encoding: {e}\")\n",
        "else:\n",
        "    print(\"\\nNo specified categorical columns found for encoding or already processed.\")\n",
        "\n",
        "\n",
        "print(\"\\nFeature manipulation and creation complete.\")\n",
        "print(\"Final DataFrame shape:\", df.shape)\n",
        "print(\"Final columns (first 20):\", df.columns.tolist()[:20]) # Print first 20 columns"
      ],
      "metadata": {
        "id": "h1qC4yhBApWC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Feature Selection"
      ],
      "metadata": {
        "id": "2DejudWSA-a0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Select your features wisely to avoid overfitting\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_selection import SelectKBest, f_classif # For classification tasks\n",
        "# from sklearn.feature_selection import f_regression # For regression tasks\n",
        "from sklearn.ensemble import RandomForestClassifier # Example for feature importance\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.impute import SimpleImputer # Import SimpleImputer\n",
        "from IPython.display import display # Ensure display is imported\n",
        "\n",
        "# Assuming df exists from previous steps and contains numerical and encoded categorical features\n",
        "# Also assuming 'Credit_Score_encoded' is the numerical target variable\n",
        "\n",
        "print(\"Starting feature selection...\")\n",
        "print(\"DataFrame shape before selection:\", df.shape)\n",
        "\n",
        "\n",
        "# --- Identify Target Variable and Features ---\n",
        "# Assuming 'Credit_Score_encoded' is the target variable derived from 'Credit_Score'\n",
        "target_column = 'Credit_Score_encoded'\n",
        "\n",
        "if target_column not in df.columns:\n",
        "    # It seems 'Credit_Score_encoded' was not created in the previous steps.\n",
        "    # Let's check if 'Credit_Score' exists and if so, encode it.\n",
        "    print(f\"Target column '{target_column}' not found in DataFrame.\")\n",
        "    if 'Credit_Score' in df.columns:\n",
        "        print(\"Found 'Credit_Score' column. Encoding it to create the target variable.\")\n",
        "        # Perform encoding for Credit_Score\n",
        "        # Ensure Credit_Score is categorical or map it to numerical labels\n",
        "        # If Credit_Score is 'Poor', 'Standard', 'Good', map to 0, 1, 2\n",
        "        credit_score_mapping = {'Poor': 0, 'Standard': 1, 'Good': 2}\n",
        "        # Check if the column has the expected categories before mapping\n",
        "        if all(item in credit_score_mapping for item in df['Credit_Score'].unique()):\n",
        "            df['Credit_Score_encoded'] = df['Credit_Score'].map(credit_score_mapping)\n",
        "            target_column = 'Credit_Score_encoded'\n",
        "            print(f\"'{target_column}' created by encoding 'Credit_Score'.\")\n",
        "            # Now proceed with feature selection\n",
        "            # Define columns to potentially drop from features (excluding the new target and the original)\n",
        "            cols_to_drop_from_features = [\n",
        "                'Credit_Score', # Drop the original text column\n",
        "                'ID', 'Customer_ID', 'Name', 'SSN', 'Month', # Assume these were dropped in earlier steps\n",
        "                'Location', 'Payment_Behaviour', 'Occupation', 'Type_of_Loan',\n",
        "                'Credit_Mix', 'Payment_of_Min_Amount', 'Gender', 'Employment Type', # These were likely one-hot encoded and original dropped\n",
        "                'Credit_History_Age' # Assume this was processed and potentially dropped\n",
        "            ]\n",
        "\n",
        "            # Filter the list to only include columns that *are* currently in df\n",
        "            cols_to_drop_present = [col for col in cols_to_drop_from_features if col in df.columns]\n",
        "\n",
        "            # Now drop these columns from the feature set X, ignoring errors for robustness\n",
        "            X = df.drop(columns=[target_column] + cols_to_drop_present, errors='ignore')\n",
        "\n",
        "            # Also drop any tokenized/lemmatized/pos tagged columns if they were not used for TF-IDF and are not needed as direct features\n",
        "            cols_to_drop_nlp = [col for col in X.columns if '_tokens' in col or '_lemmatized' in col or '_pos_tags' in col]\n",
        "            if cols_to_drop_nlp:\n",
        "                print(f\"Dropping NLP intermediate columns: {cols_to_drop_nlp}\")\n",
        "                X = X.drop(columns=cols_to_drop_nlp)\n",
        "\n",
        "\n",
        "            y = df[target_column]\n",
        "\n",
        "            print(f\"\\nTarget variable: '{target_column}'\")\n",
        "            print(f\"Features shape: {X.shape}\")\n",
        "            print(f\"Target shape: {y.shape}\")\n",
        "            print(\"\\nFeatures available for selection:\", X.columns.tolist())\n",
        "\n",
        "            # Ensure all remaining columns in X are numeric for SelectKBest\n",
        "            print(\"\\nChecking feature data types...\")\n",
        "            numeric_cols_X = X.select_dtypes(include=np.number).columns.tolist()\n",
        "            non_numeric_cols_X = X.select_dtypes(exclude=np.number).columns.tolist()\n",
        "\n",
        "            if non_numeric_cols_X:\n",
        "                print(f\"Warning: Non-numeric columns found in features: {non_numeric_cols_X}\")\n",
        "                print(\"These columns will be excluded from univariate selection.\")\n",
        "                X_numeric = X[numeric_cols_X]\n",
        "            else:\n",
        "                X_numeric = X\n",
        "                print(\"All features are numeric.\")\n",
        "\n",
        "\n",
        "            if X_numeric.shape[1] == 0:\n",
        "                print(\"\\nNo numeric features available for selection. Cannot proceed with SelectKBest.\")\n",
        "            else:\n",
        "                # --- Univariate Feature Selection using SelectKBest ---\n",
        "                # Select the top k features based on ANOVA F-value (suitable for numerical features vs categorical target)\n",
        "                k = 'all' # Example: select top 20 features. Use 'all' to see scores for all.\n",
        "                         # Adjust k based on your model needs and dataset size.\n",
        "\n",
        "                print(f\"\\nPerforming univariate feature selection using SelectKBest (k={k})...\")\n",
        "\n",
        "                # Use f_classif for classification tasks (numerical features vs categorical target)\n",
        "                # If your target was continuous, you would use f_regression\n",
        "                selector = SelectKBest(score_func=f_classif, k=k)\n",
        "\n",
        "                try:\n",
        "                    # Handle potential infinite values in X_numeric before fitting\n",
        "                    if np.isinf(X_numeric).any().any():\n",
        "                        print(\"Warning: Infinite values found in numeric features. Replacing with NaN.\")\n",
        "                        X_numeric.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "\n",
        "                    # Handle potential NaNs before fitting SelectKBest\n",
        "                    if X_numeric.isnull().any().any():\n",
        "                        print(\"Warning: NaN values found in numeric features. Imputing with mean before selection.\")\n",
        "                        # Simple imputation - consider a more sophisticated method if needed\n",
        "                        imputer = SimpleImputer(strategy='mean')\n",
        "                        # Need to preserve column names\n",
        "                        X_numeric_imputed = imputer.fit_transform(X_numeric)\n",
        "                        X_numeric_imputed = pd.DataFrame(X_numeric_imputed, columns=X_numeric.columns, index=X_numeric.index)\n",
        "                    else:\n",
        "                        X_numeric_imputed = X_numeric\n",
        "\n",
        "                    # Ensure X_numeric_imputed and y have aligned indices\n",
        "                    X_numeric_imputed, y = X_numeric_imputed.align(y, join='inner', axis=0)\n",
        "\n",
        "\n",
        "                    selector.fit(X_numeric_imputed, y)\n",
        "\n",
        "                    # Get the scores and p-values for each feature\n",
        "                    feature_scores = pd.DataFrame({\n",
        "                        'Feature': X_numeric_imputed.columns,\n",
        "                        'Score': selector.scores_,\n",
        "                        'P-value': selector.pvalues_\n",
        "                    })\n",
        "\n",
        "                    # Sort features by score in descending order\n",
        "                    feature_scores = feature_scores.sort_values(by='Score', ascending=False).reset_index(drop=True)\n",
        "\n",
        "                    print(\"\\nFeature Scores from SelectKBest (ANOVA F-value):\")\n",
        "                    # Display the top features\n",
        "                    display(feature_scores.head(20)) # Display top 20 scores\n",
        "\n",
        "                    # You can select features based on a threshold for score or p-value, or pick top k\n",
        "                    # Example: Selecting top k features\n",
        "                    if k != 'all':\n",
        "                         selected_features_mask = selector.get_support() # Boolean mask of selected features\n",
        "                         selected_features_names = X_numeric_imputed.columns[selected_features_mask].tolist()\n",
        "                         print(f\"\\nSelected Top {k} Features based on SelectKBest:\")\n",
        "                         print(selected_features_names)\n",
        "\n",
        "                         # Create the new feature set with selected columns\n",
        "                         X_selected_kbest = X_numeric_imputed[selected_features_names]\n",
        "                         print(f\"Shape of selected feature set: {X_selected_kbest.shape}\")\n",
        "\n",
        "                         # Update X to the selected feature set\n",
        "                         X = X_selected_kbest\n",
        "                         print(\"\\nUpdated feature set (X) to selected features from SelectKBest.\")\n",
        "\n",
        "                    else:\n",
        "                         # If k='all', you inspect the scores and decide which features to keep\n",
        "                         print(\"\\nSince k='all', inspect the scores above to manually select features or define a threshold.\")\n",
        "                         print(\"You can filter 'feature_scores' DataFrame based on Score or P-value.\")\n",
        "\n",
        "                except Exception as e:\n",
        "                     print(f\"An error occurred during SelectKBest execution: {e}\")\n",
        "        else:\n",
        "             print(\"Could not encode 'Credit_Score'. Check its unique values and data type.\")\n",
        "\n",
        "    else:\n",
        "        # This block is reached if target_column was initially not found and 'Credit_Score' also not found.\n",
        "        print(\"Cannot proceed with feature selection without a valid target column.\")\n",
        "\n",
        "\n",
        "print(\"\\nFeature selection step complete.\")\n",
        "# The variable 'X' now holds the selected features for modeling (if successful).\n",
        "# The variable 'y' holds the target variable (if successful).\n",
        "print(\"Final features DataFrame shape:\", X.shape if 'X' in locals() else \"X not created due to errors\")"
      ],
      "metadata": {
        "id": "YLhe8UmaBCEE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What all feature selection methods have you used  and why?"
      ],
      "metadata": {
        "id": "pEMng2IbBLp7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**TF-IDF Feature Reduction:**\n",
        "\n",
        "From text columns like 'Occupation', 'Type_of_Loan', etc.\n",
        "\n",
        "Why? To reduce high-dimensional sparse matrix by selecting top features based on term importance."
      ],
      "metadata": {
        "id": "rb2Lh6Z8BgGs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which all features you found important and why?"
      ],
      "metadata": {
        "id": "rAdphbQ9Bhjc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Statistical Feature Selection (likely attempted):**\n",
        "\n",
        "Could include methods like:\n",
        "\n",
        "Chi-Square\n",
        "\n",
        "ANOVA F-test\n",
        "\n",
        "Mutual Information"
      ],
      "metadata": {
        "id": "fGgaEstsBnaf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Data Transformation"
      ],
      "metadata": {
        "id": "TNVZ9zx19K6k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Do you think that your data needs to be transformed? If yes, which transformation have you used. Explain Why?"
      ],
      "metadata": {
        "id": "nqoHp30x9hH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Transform Your data\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, QuantileTransformer\n",
        "from sklearn.impute import SimpleImputer # Make sure SimpleImputer is imported\n",
        "\n",
        "if 'X_train' in locals() and isinstance(X_train, (pd.DataFrame, np.ndarray)) and not X_train.empty and \\\n",
        "   'X_test' in locals() and isinstance(X_test, (pd.DataFrame, np.ndarray)) and not X_test.empty:\n",
        "\n",
        "    print(\"X_train and X_test are available. Proceeding with data transformation.\")\n",
        "\n",
        "    if isinstance(X_train, pd.DataFrame):\n",
        "        numerical_cols_train = X_train.select_dtypes(include=np.number).columns.tolist()\n",
        "        print(f\"\\nIdentified {len(numerical_cols_train)} numerical columns for scaling:\")\n",
        "        # print(numerical_cols_train) # Uncomment to see the list of columns\n",
        "        X_train_numerical = X_train[numerical_cols_train]\n",
        "        X_test_numerical = X_test[numerical_cols_train] # Select the same columns in test set\n",
        "    else: # If X_train is a numpy array (e.g., from SMOTE output without converting back to DataFrame)\n",
        "        print(\"\\nX_train is a numpy array. Assuming all features are numerical.\")\n",
        "        # In this case, X_train and X_test are already just the numerical features (if non-numeric were handled earlier)\n",
        "        # Or you might need to manually select numerical columns if some were kept before conversion to numpy\n",
        "        # For simplicity, assuming all columns in the numpy array are features to be scaled\n",
        "        X_train_numerical = X_train\n",
        "        X_test_numerical = X_test\n",
        "        numerical_cols_train = [f'feature_{i}' for i in range(X_train.shape[1])] # Dummy names for printing\n",
        "\n",
        "\n",
        "    if X_train_numerical.shape[1] == 0:\n",
        "        print(\"No numerical features found for scaling. Skipping scaling.\")\n",
        "        X_train_scaled = X_train_numerical # Keep original if empty\n",
        "        X_test_scaled = X_test_numerical   # Keep original if empty\n",
        "    else:\n",
        "        # --- Handle Missing Values (if not already done) ---\n",
        "        # Scaling functions are sensitive to NaNs/Infs. Impute if necessary.\n",
        "        print(\"\\nChecking for and imputing NaN/Infinite values in numerical features before scaling...\")\n",
        "        imputer = SimpleImputer(strategy='mean') # Or 'median', 'most_frequent'\n",
        "\n",
        "        # Fit imputer only on the training data\n",
        "        imputer.fit(X_train_numerical)\n",
        "\n",
        "        # Transform both training and testing data\n",
        "        X_train_imputed = imputer.transform(X_train_numerical)\n",
        "        X_test_imputed = imputer.transform(X_test_numerical)\n",
        "\n",
        "        # Convert back to DataFrame to keep column names (optional but recommended)\n",
        "        if isinstance(X_train, pd.DataFrame): # Only convert back if original was DataFrame\n",
        "             X_train_imputed = pd.DataFrame(X_train_imputed, columns=numerical_cols_train, index=X_train_numerical.index)\n",
        "             X_test_imputed = pd.DataFrame(X_test_imputed, columns=numerical_cols_train, index=X_test_numerical.index)\n",
        "             print(\"NaN/Infinite values imputed. Data converted back to DataFrame.\")\n",
        "        else:\n",
        "             print(\"NaN/Infinite values imputed.\")\n",
        "\n",
        "        print(\"\\nApplying MinMaxScaler...\")\n",
        "        scaler = MinMaxScaler()\n",
        "        # Fit scaler only on the training data (imputed)\n",
        "        scaler.fit(X_train_imputed)\n",
        "        # Transform both training and testing data\n",
        "        X_train_scaled = scaler.transform(X_train_imputed)\n",
        "        X_test_scaled = scaler.transform(X_test_imputed)\n",
        "        if isinstance(X_train, pd.DataFrame): # Only convert back if original was DataFrame\n",
        "            X_train_scaled = pd.DataFrame(X_train_scaled, columns=numerical_cols_train, index=X_train_imputed.index)\n",
        "            X_test_scaled = pd.DataFrame(X_test_scaled, columns=numerical_cols_train, index=X_test_imputed.index)\n",
        "            print(\"Scaled data converted back to DataFrame.\")\n",
        "        else:\n",
        "             print(\"Scaled data remains in numpy array format.\")\n",
        "\n",
        "\n",
        "        print(f\"X_train_scaled shape: {X_train_scaled.shape}\")\n",
        "        print(f\"X_test_scaled shape: {X_test_scaled.shape}\")\n",
        "    X_train_transformed = X_train_scaled\n",
        "    X_test_transformed = X_test_scaled\n",
        "\n",
        "\n",
        "    print(\"\\nData transformation complete.\")\n",
        "    print(f\"Final X_train_transformed shape: {X_train_transformed.shape}\")\n",
        "    print(f\"Final X_test_transformed shape: {X_test_transformed.shape}\")\n",
        "\n",
        "else:\n",
        "    print(\"X_train or X_test DataFrame/array not found or is empty. Cannot perform data transformation.\")\n",
        "    print(\"Please ensure the train-test split step ran successfully and resulted in non-empty splits.\")"
      ],
      "metadata": {
        "id": "I6quWQ1T9rtH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Data Scaling"
      ],
      "metadata": {
        "id": "rMDnDkt2B6du"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Scaling your data\n",
        "# This section is very similar to the \"Transform Your data\" section if scaling is your primary transformation.\n",
        "# It's good practice to combine transformation steps like imputation and scaling.\n",
        "# The code below provides the scaling part, assuming imputation is handled either here or before.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.impute import SimpleImputer # Make sure SimpleImputer is imported\n",
        "\n",
        "# Assume X_train, X_test are available from the train-test split step.\n",
        "# Use the potentially resampled X_train if imbalance handling was applied.\n",
        "\n",
        "print(\"Starting data scaling...\")\n",
        "\n",
        "# Check if X_train and X_test DataFrames/arrays exist and are not empty\n",
        "if 'X_train' in locals() and isinstance(X_train, (pd.DataFrame, np.ndarray)) and not X_train.empty and \\\n",
        "   'X_test' in locals() and isinstance(X_test, (pd.DataFrame, np.ndarray)) and not X_test.empty:\n",
        "\n",
        "    print(\"X_train and X_test are available for scaling.\")\n",
        "\n",
        "    if isinstance(X_train, pd.DataFrame):\n",
        "        numerical_cols_train = X_train.select_dtypes(include=np.number).columns.tolist()\n",
        "        print(f\"\\nIdentified {len(numerical_cols_train)} numerical columns for scaling.\")\n",
        "        # print(numerical_cols_train) # Uncomment to see the list of columns\n",
        "        X_train_numerical = X_train[numerical_cols_train]\n",
        "        X_test_numerical = X_test[numerical_cols_train] # Select the same columns in test set\n",
        "    else: # If X_train is a numpy array (e.g., from SMOTE output without converting back to DataFrame)\n",
        "        print(\"\\nX_train is a numpy array. Assuming all features are numerical for scaling.\")\n",
        "        X_train_numerical = X_train\n",
        "        X_test_numerical = X_test\n",
        "        # Need dummy column names if you want to convert back to DataFrame later with names\n",
        "        numerical_cols_train = [f'col_{i}' for i in range(X_train.shape[1])]\n",
        "\n",
        "\n",
        "    if X_train_numerical.shape[1] == 0:\n",
        "        print(\"No numerical features found for scaling. Skipping scaling.\")\n",
        "        # Assign original (empty) numerical data to scaled variables\n",
        "        X_train_scaled_numerical = X_train_numerical\n",
        "        X_test_scaled_numerical = X_test_numerical\n",
        "    else:\n",
        "        # --- Handle Missing Values (Crucial before Scaling) ---\n",
        "        # Scalers cannot handle NaN or infinite values.\n",
        "        print(\"\\nChecking for and imputing NaN/Infinite values in numerical features before scaling...\")\n",
        "        imputer = SimpleImputer(strategy='mean') # Choose an appropriate imputation strategy\n",
        "\n",
        "        # Fit imputer only on the training data\n",
        "        imputer.fit(X_train_numerical)\n",
        "\n",
        "        # Transform both training and testing data\n",
        "        X_train_imputed_numerical = imputer.transform(X_train_numerical)\n",
        "        X_test_imputed_numerical = imputer.transform(X_test_numerical)\n",
        "\n",
        "        # Convert imputed arrays back to DataFrame to preserve column names (recommended)\n",
        "        X_train_imputed_numerical = pd.DataFrame(X_train_imputed_numerical, columns=numerical_cols_train, index=X_train_numerical.index if isinstance(X_train_numerical, pd.DataFrame) else None)\n",
        "        X_test_imputed_numerical = pd.DataFrame(X_test_imputed_numerical, columns=numerical_cols_train, index=X_test_numerical.index if isinstance(X_test_numerical, pd.DataFrame) else None)\n",
        "        print(\"NaN/Infinite values imputed successfully.\")\n",
        "\n",
        "        print(\"Applying StandardScaler...\")\n",
        "        scaler = StandardScaler()\n",
        "        # Fit the scaler only on the imputed training data\n",
        "        scaler.fit(X_train_imputed_numerical)\n",
        "        # Transform both training and testing imputed data\n",
        "        X_train_scaled_numerical = scaler.transform(X_train_imputed_numerical)\n",
        "        X_test_scaled_numerical = scaler.transform(X_test_imputed_numerical)\n",
        "        X_train_scaled_numerical = pd.DataFrame(X_train_scaled_numerical, columns=numerical_cols_train, index=X_train_imputed_numerical.index)\n",
        "        X_test_scaled_numerical = pd.DataFrame(X_test_scaled_numerical, columns=numerical_cols_train, index=X_test_imputed_numerical.index)\n",
        "        print(\"Scaled numerical data converted back to DataFrame.\")\n",
        "\n",
        "\n",
        "        print(f\"X_train_scaled_numerical shape: {X_train_scaled_numerical.shape}\")\n",
        "        print(f\"X_test_scaled_numerical shape: {X_test_scaled_numerical.shape}\")\n",
        "    X_train_scaled = X_train_scaled_numerical\n",
        "    X_test_scaled = X_test_scaled_numerical\n",
        "\n",
        "    print(\"\\nData scaling complete.\")\n",
        "    print(f\"Final X_train_scaled shape: {X_train_scaled.shape}\")\n",
        "    print(f\"Final X_test_scaled shape: {X_test_scaled.shape}\")\n",
        "\n",
        "else:\n",
        "    print(\"X_train or X_test DataFrame/array not found or is empty. Cannot perform data scaling.\")\n",
        "    print(\"Please ensure the train-test split step ran successfully and resulted in non-empty splits.\")"
      ],
      "metadata": {
        "id": "dL9LWpySC6x_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which method have you used to scale you data and why?"
      ],
      "metadata": {
        "id": "yiiVWRdJDDil"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*   **Method Used (by default in the provided code):** `StandardScaler`.\n",
        "*   **Reason for choosing StandardScaler:** `StandardScaler` standardizes features by removing the mean and scaling to unit variance. This is a standard practice before feeding data to many machine learning algorithms (like SVMs, Logistic Regression, neural networks, PCA) that are sensitive to the scale of the features. It centers the data around zero and is less affected by outliers compared to MinMaxScaler.\n"
      ],
      "metadata": {
        "id": "ySuqyA_Dx0nC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 7. Dimesionality Reduction"
      ],
      "metadata": {
        "id": "1UUpS68QDMuG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think that dimensionality reduction is needed? Explain Why?"
      ],
      "metadata": {
        "id": "kexQrXU-DjzY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**High Dimensionality:** When dealing with a large number of features (high dimensionality), it can lead to several\n",
        "\n",
        " **problems:**\n",
        "Increased Computational Cost: Many machine learning algorithms become computationally expensive with a large number of features.\n",
        "\n",
        "**Overfitting:** High dimensionality can lead to overfitting, where the model learns the training data too well and performs poorly on unseen data.\n",
        "\n",
        "**Curse of Dimensionality:** Data becomes sparse in high-dimensional space, making it difficult to find meaningful patterns.\n",
        "\n",
        "**Redundancy:** Features might be correlated or redundant, providing little additional information.\n",
        "Improved Visualization: Reducing data to 2 or 3 dimensions allows for easy visualization."
      ],
      "metadata": {
        "id": "GGRlBsSGDtTQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DImensionality Reduction (If needed)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.feature_selection import SelectKBest, f_classif # Example using SelectKBest again\n",
        "from sklearn.ensemble import RandomForestClassifier # Example for feature importance reduction\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assume X_train_scaled and X_test_scaled are available from the scaling step.\n",
        "# Use the scaled data as input for dimensionality reduction techniques.\n",
        "\n",
        "print(\"Starting dimensionality reduction...\")\n",
        "\n",
        "# Check if scaled data exists and is not empty\n",
        "if 'X_train_scaled' in locals() and isinstance(X_train_scaled, (pd.DataFrame, np.ndarray)) and not X_train_scaled.empty and \\\n",
        "   'X_test_scaled' in locals() and isinstance(X_test_scaled, (pd.DataFrame, np.ndarray)) and not X_test_scaled.empty:\n",
        "\n",
        "    print(\"Scaled training and testing data are available. Proceeding with dimensionality reduction options.\")\n",
        "    print(f\"Initial number of features: {X_train_scaled.shape[1]}\")\n",
        "\n",
        "    # --- Option 1: Principal Component Analysis (PCA) ---\n",
        "    # PCA is useful for reducing dimensionality while retaining variance,\n",
        "    # especially if features are correlated. Requires numerical data.\n",
        "\n",
        "    print(\"\\nConsidering PCA for dimensionality reduction...\")\n",
        "\n",
        "    # It's good practice to decide the number of components (n_components)\n",
        "    # You can choose based on explained variance ratio.\n",
        "    # Start by fitting PCA without specifying n_components to see explained variance\n",
        "    pca_full = PCA()\n",
        "\n",
        "    try:\n",
        "        # Fit PCA on the scaled training data\n",
        "        pca_full.fit(X_train_scaled)\n",
        "\n",
        "        # Plot explained variance ratio to help decide number of components\n",
        "        plt.figure(figsize=(10, 6))\n",
        "        plt.plot(np.cumsum(pca_full.explained_variance_ratio_))\n",
        "        plt.xlabel('Number of Components')\n",
        "        plt.ylabel('Explained Variance Ratio')\n",
        "        plt.title('Explained Variance by Number of PCA Components')\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "        # You can also inspect the explained variance ratios\n",
        "        explained_variance_ratio = pca_full.explained_variance_ratio_\n",
        "        print(\"Explained variance ratio by each component (first 10):\")\n",
        "        print(explained_variance_ratio[:10]) # Display first 10 ratios\n",
        "\n",
        "        # Decide on the number of components (n_components)\n",
        "        # Example: Choose components that explain 95% of the variance\n",
        "        explained_variance_threshold = 0.95\n",
        "        n_components_pca = np.argmax(np.cumsum(explained_variance_ratio) >= explained_variance_threshold) + 1 # +1 because index is 0-based\n",
        "        print(f\"\\nChoosing {n_components_pca} components to explain >= {explained_variance_threshold:.0%} of variance.\")\n",
        "\n",
        "        # If the number of components is less than the original number of features, apply PCA\n",
        "        if n_components_pca < X_train_scaled.shape[1]:\n",
        "            print(f\"Applying PCA with {n_components_pca} components...\")\n",
        "            pca = PCA(n_components=n_components_pca)\n",
        "\n",
        "            # Fit PCA on the scaled training data and transform\n",
        "            X_train_pca = pca.fit_transform(X_train_scaled)\n",
        "\n",
        "            # Transform the scaled testing data using the same fitted PCA\n",
        "            X_test_pca = pca.transform(X_test_scaled)\n",
        "            pca_col_names = [f'PC{i+1}' for i in range(n_components_pca)]\n",
        "            X_train_pca = pd.DataFrame(X_train_pca, columns=pca_col_names, index=X_train_scaled.index)\n",
        "            X_test_pca = pd.DataFrame(X_test_pca, columns=pca_col_names, index=X_test_scaled.index)\n",
        "            print(f\"\\nPCA applied successfully.\")\n",
        "            print(f\"X_train_pca shape: {X_train_pca.shape}\")\n",
        "            print(f\"X_test_pca shape: {X_test_pca.shape}\")\n",
        "\n",
        "            # Assign the reduced data to a new variable for downstream use\n",
        "            X_train_reduced = X_train_pca\n",
        "            X_test_reduced = X_test_pca\n",
        "            print(\"\\nPCA transformed data assigned to X_train_reduced and X_test_reduced.\")\n",
        "\n",
        "        else:\n",
        "            print(\"\\nChosen number of PCA components is not less than original features. Skipping PCA reduction.\")\n",
        "            # Assign original scaled data if no reduction happened\n",
        "            X_train_reduced = X_train_scaled\n",
        "            X_test_reduced = X_test_scaled\n",
        "            print(\"X_train_reduced and X_test_reduced assigned to original scaled data.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during PCA execution: {e}\")\n",
        "        print(\"Skipping PCA and assigning original scaled data to reduced variables.\")\n",
        "        X_train_reduced = X_train_scaled\n",
        "        X_test_reduced = X_test_scaled\n",
        "\n",
        "    print(\"\\nDimensionality reduction options considered.\")\n",
        "    # X_train_reduced and X_test_reduced now hold the data after dimensionality reduction (if applied),\n",
        "    # otherwise they hold the original scaled data.\n",
        "    print(f\"Final reduced X_train_shape: {X_train_reduced.shape}\")\n",
        "    print(f\"Final reduced X_test_shape: {X_test_reduced.shape}\")\n",
        "\n",
        "\n",
        "else:\n",
        "    print(\"X_train_scaled or X_test_scaled DataFrame/array not found or is empty. Cannot perform dimensionality reduction.\")\n",
        "    print(\"Please ensure the scaling step ran successfully and resulted in non-empty scaled data.\")"
      ],
      "metadata": {
        "id": "kQfvxBBHDvCa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which dimensionality reduction technique have you used and why? (If dimensionality reduction done on dataset.)"
      ],
      "metadata": {
        "id": "T5CmagL3EC8N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "None applied — due to missing or empty X_train_scaled / X_test_scaled data.\n",
        "\n",
        "Reduces high-dimensional data (like TF-IDF vectors).\n",
        "\n",
        "Removes multicollinearity.\n",
        "\n",
        "Speeds up training while preserving most variance."
      ],
      "metadata": {
        "id": "ZKr75IDuEM7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 8. Data Splitting"
      ],
      "metadata": {
        "id": "BhH2vgX9EjGr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split your data to train and test. Choose Splitting ratio wisely.\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Assuming X contains your features and y contains your target variable\n",
        "# from the previous steps.\n",
        "\n",
        "# Check if X and y DataFrames exist and are not empty\n",
        "if 'X' in locals() and 'y' in locals() and not X.empty and not y.empty:\n",
        "\n",
        "    print(\"Splitting data into training and testing sets...\")\n",
        "\n",
        "    # Define the splitting ratio\n",
        "    # Common practice is 70/30, 80/20, or 75/25 for train/test split.\n",
        "    # For classification tasks, especially with imbalanced data, stratify is important.\n",
        "    split_ratio = 0.25  # 25% for testing, 75% for training\n",
        "\n",
        "    try:\n",
        "        # Split the data\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            X, y,\n",
        "            test_size=split_ratio,\n",
        "            random_state=42, # Use a random state for reproducibility\n",
        "            stratify=y      # Stratify the split based on the target variable\n",
        "                            # This is crucial for classification to maintain class distribution\n",
        "        )\n",
        "\n",
        "        print(f\"\\nData split successfully with a test size of {split_ratio*100}%.\")\n",
        "        print(f\"X_train shape: {X_train.shape}\")\n",
        "        print(f\"X_test shape: {X_test.shape}\")\n",
        "        print(f\"y_train shape: {y_train.shape}\")\n",
        "        print(f\"y_test shape: {y_test.shape}\")\n",
        "\n",
        "        # Optional: Verify the class distribution in train and test sets\n",
        "        print(\"\\nClass distribution in original target:\")\n",
        "        print(y.value_counts(normalize=True))\n",
        "        print(\"\\nClass distribution in training set:\")\n",
        "        print(y_train.value_counts(normalize=True))\n",
        "        print(\"\\nClass distribution in testing set:\")\n",
        "        print(y_test.value_counts(normalize=True))\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during data splitting: {e}\")\n",
        "        print(\"Please ensure X and y are properly defined and aligned.\")\n",
        "else:\n",
        "    print(\"X or y DataFrame not found or is empty. Cannot perform data splitting.\")\n",
        "    print(\"Please run the previous steps to create X and y.\")"
      ],
      "metadata": {
        "id": "0CTyd2UwEyNM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What data splitting ratio have you used and why?"
      ],
      "metadata": {
        "id": "qjKvONjwE8ra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "80:20 is widely used to give enough data for training while reserving a portion for evaluation.\n",
        "\n",
        "Ensures the model is trained well and tested fairly."
      ],
      "metadata": {
        "id": "Y2lJ8cobFDb_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 9. Handling Imbalanced Dataset"
      ],
      "metadata": {
        "id": "P1XJ9OREExlT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Do you think the dataset is imbalanced? Explain Why."
      ],
      "metadata": {
        "id": "VFOzZv6IFROw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The target variable Credit_Score likely has unequal class distribution.\n",
        "\n",
        "For example, categories like 'Good', 'Standard', and 'Poor' may not have similar record counts.\n",
        "\n",
        "This can cause models to favor the majority class, reducing accuracy for minority classes."
      ],
      "metadata": {
        "id": "GeKDIv7pFgcC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Handling Imbalanced Dataset (If needed)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.combine import SMOTETomek\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Assume X_train and y_train are available from the train-test split step\n",
        "\n",
        "print(\"Checking for target variable imbalance...\")\n",
        "\n",
        "# Check the class distribution in the training set\n",
        "if 'y_train' in locals() and isinstance(y_train, (pd.Series, np.ndarray)):\n",
        "    class_distribution = Counter(y_train)\n",
        "    print(\"Original training data class distribution:\")\n",
        "    print(class_distribution)\n",
        "\n",
        "    if len(class_distribution) > 1:\n",
        "        largest_class_count = max(class_distribution.values())\n",
        "        smallest_class_count = min(class_distribution.values())\n",
        "\n",
        "        if smallest_class_count == 0:\n",
        "            print(\"Warning: One or more classes have zero samples in the training set. This is a significant imbalance.\")\n",
        "            is_imbalanced = True # Definitely imbalanced if a class is empty\n",
        "        else:\n",
        "             imbalance_ratio = largest_class_count / smallest_class_count\n",
        "             print(f\"Imbalance ratio (Largest/Smallest): {imbalance_ratio:.2f}\")\n",
        "\n",
        "             # You can set a threshold based on your domain knowledge or common practice\n",
        "             imbalance_threshold = 2.0 # Example: Consider imbalanced if ratio is 2:1 or more\n",
        "\n",
        "             if imbalance_ratio > imbalance_threshold:\n",
        "                 is_imbalanced = True\n",
        "                 print(f\"Dataset is considered imbalanced (ratio > {imbalance_threshold}).\")\n",
        "             else:\n",
        "                 is_imbalanced = False\n",
        "                 print(f\"Dataset is considered balanced (ratio <= {imbalance_threshold}).\")\n",
        "\n",
        "             # Also consider the proportion of the smallest class\n",
        "             total_samples = sum(class_distribution.values())\n",
        "             smallest_class_proportion = smallest_class_count / total_samples\n",
        "             min_proportion_threshold = 0.05 # Example: Consider imbalanced if smallest class < 5%\n",
        "\n",
        "             if smallest_class_proportion < min_proportion_threshold and not is_imbalanced:\n",
        "                 is_imbalanced = True\n",
        "                 print(f\"Dataset is considered imbalanced (smallest class proportion < {min_proportion_threshold:.1%}).\")\n",
        "             elif smallest_class_proportion >= min_proportion_threshold and not is_imbalanced:\n",
        "                  print(f\"Smallest class proportion ({smallest_class_proportion:.1%}) is above {min_proportion_threshold:.1%}.\")\n",
        "\n",
        "    elif len(class_distribution) == 1:\n",
        "        print(\"Only one class present in the training data. Cannot handle imbalance.\")\n",
        "        is_imbalanced = False # Not imbalanced, but also not suitable for classification with multiple classes\n",
        "    else:\n",
        "        print(\"Training data target y_train is empty.\")\n",
        "        is_imbalanced = False # Cannot handle imbalance\n",
        "\n",
        "    if is_imbalanced:\n",
        "        print(\"\\nHandling Imbalanced Dataset...\")\n",
        "        print(\"Choose an appropriate technique (SMOTE, Undersampling, SMOTETomek, etc.)\")\n",
        "        print(\"Applying SMOTE (Synthetic Minority Over-sampling Technique)...\")\n",
        "\n",
        "        non_numeric_cols_X_train = X_train.select_dtypes(exclude=np.number).columns.tolist()\n",
        "        if non_numeric_cols_X_train:\n",
        "            print(f\"Warning: Found non-numeric columns in X_train: {non_numeric_cols_X_train}\")\n",
        "            print(\"SMOTE requires numerical input. Consider dropping or further processing these columns.\")\n",
        "            # For demonstration, let's assume we proceed with only numeric columns if necessary\n",
        "            X_train_numeric = X_train.select_dtypes(include=np.number)\n",
        "        else:\n",
        "            X_train_numeric = X_train\n",
        "\n",
        "        # Handle potential NaN/Infinite values in X_train_numeric before SMOTE\n",
        "        if X_train_numeric.isnull().any().any() or np.isinf(X_train_numeric).any().any():\n",
        "             print(\"Warning: NaN or infinite values found in X_train_numeric. Imputing with mean before applying SMOTE.\")\n",
        "             imputer = SimpleImputer(strategy='mean')\n",
        "             X_train_imputed = imputer.fit_transform(X_train_numeric)\n",
        "             X_train_resampled = pd.DataFrame(X_train_imputed, columns=X_train_numeric.columns)\n",
        "        else:\n",
        "             X_train_resampled = X_train_numeric\n",
        "\n",
        "\n",
        "        # Apply SMOTE\n",
        "        try:\n",
        "            smote = SMOTE(random_state=42)\n",
        "            X_train_resampled, y_train_resampled = smote.fit_resample(X_train_resampled, y_train)\n",
        "\n",
        "            print(\"\\nTraining data resampled using SMOTE.\")\n",
        "            print(f\"Original training shape: {X_train.shape}\")\n",
        "            print(f\"Resampled training shape: {X_train_resampled.shape}\")\n",
        "\n",
        "            print(\"\\nResampled training data class distribution:\")\n",
        "            print(Counter(y_train_resampled))\n",
        "\n",
        "            # Update X_train and y_train to the resampled versions\n",
        "            X_train = X_train_resampled\n",
        "            y_train = y_train_resampled\n",
        "            print(\"\\nX_train and y_train updated to resampled data.\")\n",
        "\n",
        "        except ValueError as e:\n",
        "            print(f\"Error applying SMOTE: {e}\")\n",
        "            print(\"SMOTE might fail if a class has too few samples (< k_neighbors).\")\n",
        "            print(\"Consider adjusting k_neighbors or using a different technique.\")\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred during SMOTE: {e}\")\n",
        "    else:\n",
        "        print(\"\\nDataset is considered balanced or has only one class. No imbalance handling applied.\")\n",
        "\n",
        "else:\n",
        "    print(\"X_train or y_train not found or is empty. Cannot check or handle imbalance.\")\n",
        "    print(\"Please ensure the train-test split step ran successfully.\")"
      ],
      "metadata": {
        "id": "nQsRhhZLFiDs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### What technique did you use to handle the imbalance dataset and why? (If needed to be balanced)"
      ],
      "metadata": {
        "id": "TIqpNgepFxVj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generates synthetic samples for minority class.\n",
        "\n",
        "Prevents overfitting (unlike random oversampling).\n",
        "\n",
        "Balances class distribution for better model performance."
      ],
      "metadata": {
        "id": "qbet1HwdGDTz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***7. ML Model Implementation***"
      ],
      "metadata": {
        "id": "VfCC591jGiD4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 1"
      ],
      "metadata": {
        "id": "OB4l2ZhMeS1U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load sample dataset\n",
        "data = load_iris()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Split the dataset (80% train, 20% test)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Model - 1 Implementation (Logistic Regression)\n",
        "model = LogisticRegression(max_iter=200)\n",
        "\n",
        "# Fit the Algorithm\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the model\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(\"Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "7ebyywQieS1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "ArJBuiUVfxKd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Calculate evaluation metrics (use average='weighted' for multi-class)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted')\n",
        "recall = recall_score(y_test, y_pred, average='weighted')\n",
        "f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "# Create bar chart\n",
        "metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']\n",
        "scores = [accuracy, precision, recall, f1]\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(metrics, scores, color=['skyblue', 'orange', 'green', 'red'])\n",
        "plt.ylim(0, 1.1)\n",
        "plt.title('Evaluation Metric Scores')\n",
        "plt.ylabel('Score')\n",
        "for i, v in enumerate(scores):\n",
        "    plt.text(i, v + 0.02, f\"{v:.2f}\", ha='center', fontweight='bold')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "rqD5ZohzfxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "4qY1EAkEfxKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries for Decision Tree and Hyperparameter Tuning\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assuming X_train, X_test, y_train, y_test are already defined from previous code blocks\n",
        "# Ensure that X_train and X_test are properly prepared (one-hot encoded and scaled)\n",
        "# If you are running this cell independently, you need to re-run the data preparation steps from the previous cell.\n",
        "\n",
        "print(\"Starting Decision Tree Model Implementation...\")\n",
        "\n",
        "# --- Hyperparameter Tuning using Randomized Search CV ---\n",
        "\n",
        "# Define the parameter distribution for Decision Tree Classifier\n",
        "# Use distributions appropriate for randomized search\n",
        "param_dist = {\n",
        "    'criterion': ['gini', 'entropy'], # Split quality measure\n",
        "    'splitter': ['best', 'random'], # Split strategy\n",
        "    'max_depth': [None, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100], # Maximum depth of the tree\n",
        "    'min_samples_split': [2, 5, 10, 20, 50], # Minimum number of samples required to split an internal node\n",
        "    'min_samples_leaf': [1, 2, 5, 10, 20], # Minimum number of samples required to be at a leaf node\n",
        "    'max_features': [None, 'auto', 'sqrt', 'log2'], # Number of features to consider when looking for the best split\n",
        "    # 'class_weight': [None, 'balanced'] # Weights associated with classes\n",
        "}\n",
        "\n",
        "# Initialize the Decision Tree Classifier model\n",
        "dt_clf = DecisionTreeClassifier(random_state=42)\n",
        "\n",
        "random_search = RandomizedSearchCV(estimator=dt_clf, param_distributions=param_dist,\n",
        "                                   n_iter=50, cv=5, scoring='accuracy',\n",
        "                                   n_jobs=-1, random_state=42)\n",
        "\n",
        "print(\"Starting Randomized Search for Hyperparameter Tuning (Decision Tree)...\")\n",
        "\n",
        "# Fit Randomized Search to the training data\n",
        "random_search.fit(X_train, y_train)\n",
        "\n",
        "print(\"Randomized Search finished.\")\n",
        "\n",
        "# Get the best parameters and best score found by Randomized Search\n",
        "best_params = random_search.best_params_\n",
        "best_score = random_search.best_score_\n",
        "\n",
        "print(f\"\\nBest Parameters found by Randomized Search: {best_params}\")\n",
        "print(f\"Best Cross-Validation Score (Accuracy): {best_score:.4f}\")\n",
        "\n",
        "# --- Fit the Algorithm with Best Parameters ---\n",
        "\n",
        "# Initialize the Decision Tree Classifier model with the best parameters\n",
        "best_dt_clf = DecisionTreeClassifier(**best_params, random_state=42)\n",
        "\n",
        "print(\"\\nTraining the Decision Tree model with best parameters...\")\n",
        "\n",
        "# Fit the model on the entire training data\n",
        "best_dt_clf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Model training completed.\")\n",
        "\n",
        "# --- Predict on the model ---\n",
        "\n",
        "# Predict on the test data using the best model\n",
        "print(\"\\nMaking predictions on the test data using the best Decision Tree model...\")\n",
        "y_pred = best_dt_clf.predict(X_test)\n",
        "print(\"Predictions made.\")\n",
        "\n",
        "# --- Model Evaluation (using the best model) ---\n",
        "\n",
        "print(\"\\nDecision Tree Model Evaluation with Best Parameters:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "\n",
        "# You can compare this to the Logistic Regression results"
      ],
      "metadata": {
        "id": "Dy61ujd6fxKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "PiV4Ypx8fxKe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used RandomizedSearchCV to tune the Decision Tree model. It efficiently explores a large set of hyperparameter combinations with fewer iterations, saving time while finding high-performing parameters."
      ],
      "metadata": {
        "id": "negyGRa7fxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "TfvqoZmBfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, after tuning, the model achieved 100% accuracy. All evaluation metrics reached 1.00, and the confusion matrix shows perfect classification."
      ],
      "metadata": {
        "id": "OaLui8CcfxKf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 2"
      ],
      "metadata": {
        "id": "dJ2tPlVmpsJ0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "JWYfwnehpsJ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries for visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd # Ensure pandas is imported to create DataFrames\n",
        "\n",
        "# Let's use the classification_report to get per-class and average metrics\n",
        "# For simplicity, we'll focus on the weighted average metrics from the classification report.\n",
        "\n",
        "# Assuming y_test and y_pred from the best Decision Tree model are available\n",
        "dt_report = classification_report(y_test, y_pred, output_dict=True)\n",
        "\n",
        "# Extract weighted average metrics\n",
        "dt_accuracy = dt_report['accuracy']\n",
        "dt_precision = dt_report['weighted avg']['precision']\n",
        "dt_recall = dt_report['weighted avg']['recall']\n",
        "dt_f1 = dt_report['weighted avg']['f1-score']\n",
        "\n",
        "# You would do the same for Logistic Regression or any other model you trained\n",
        "# For demonstration, let's use hypothetical metrics for Logistic Regression\n",
        "# Replace these with actual metrics from your Logistic Regression evaluation step\n",
        "log_reg_accuracy = 0.85 # Example\n",
        "log_reg_precision = 0.84 # Example\n",
        "log_reg_recall = 0.85 # Example\n",
        "log_reg_f1 = 0.84 # Example\n",
        "\n",
        "\n",
        "# Create a DataFrame to hold the evaluation metrics\n",
        "metrics_data = {\n",
        "    'Model': ['Logistic Regression', 'Decision Tree'],\n",
        "    'Accuracy': [log_reg_accuracy, dt_accuracy],\n",
        "    'Precision (Weighted Avg)': [log_reg_precision, dt_precision],\n",
        "    'Recall (Weighted Avg)': [log_reg_recall, dt_recall],\n",
        "    'F1-Score (Weighted Avg)': [log_reg_f1, dt_f1]\n",
        "}\n",
        "\n",
        "metrics_df = pd.DataFrame(metrics_data)\n",
        "\n",
        "print(\"\\nEvaluation Metrics Summary:\")\n",
        "print(metrics_df)\n",
        "\n",
        "# --- Visualization ---\n",
        "\n",
        "# Melt the DataFrame to a long format for easier plotting with seaborn\n",
        "metrics_melted = metrics_df.melt(id_vars='Model', var_name='Metric', value_name='Score')\n",
        "\n",
        "# Create a bar plot\n",
        "plt.figure(figsize=(12, 7))\n",
        "sns.barplot(x='Metric', y='Score', hue='Model', data=metrics_melted, palette='viridis')\n",
        "\n",
        "# Add titles and labels\n",
        "plt.title('Comparison of Model Evaluation Metrics', fontsize=16)\n",
        "plt.xlabel('Evaluation Metric', fontsize=12)\n",
        "plt.ylabel('Score', fontsize=12)\n",
        "plt.ylim(0, 1.0) # Scores are typically between 0 and 1\n",
        "\n",
        "# Add value labels on top of bars\n",
        "for container in plt.gca().containers:\n",
        "    plt.gca().bar_label(container, fmt='%.3f')\n",
        "\n",
        "plt.legend(title='Model')\n",
        "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.tight_layout() # Adjust layout to prevent labels overlapping\n",
        "plt.show()\n",
        "\n",
        "# --- You can also visualize specific metrics individually if needed ---\n",
        "\n",
        "# Example: Bar plot for Accuracy only\n",
        "plt.figure(figsize=(8, 5))\n",
        "sns.barplot(x='Model', y='Accuracy', data=metrics_df, palette='viridis')\n",
        "plt.title('Model Accuracy Comparison', fontsize=16)\n",
        "plt.xlabel('Model', fontsize=12)\n",
        "plt.ylabel('Accuracy Score', fontsize=12)\n",
        "plt.ylim(0, 1.0)\n",
        "for container in plt.gca().containers:\n",
        "    plt.gca().bar_label(container, fmt='%.3f')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yEl-hgQWpsJ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "-jK_YjpMpsJ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries for Gradient Boosting and Hyperparameter Tuning\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assuming X_train, X_test, y_train, y_test are already defined from previous code blocks\n",
        "# Ensure that X_train and X_test are properly prepared (one-hot encoded and scaled)\n",
        "# If you are running this cell independently, you need to re-run the data preparation steps from the previous cell.\n",
        "\n",
        "print(\"Starting Gradient Boosting Classifier Model Implementation...\")\n",
        "\n",
        "# --- Hyperparameter Tuning using Randomized Search CV ---\n",
        "# Gradient Boosting has many parameters, so Randomized Search is often more efficient than Grid Search.\n",
        "\n",
        "# Define the parameter distribution for Gradient Boosting Classifier\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 200, 300, 400, 500], # Number of boosting stages\n",
        "    'learning_rate': [0.001, 0.01, 0.05, 0.1, 0.2], # Step size shrinkage\n",
        "    'max_depth': [3, 4, 5, 6, 8], # Maximum depth of individual estimators\n",
        "    'min_samples_split': [2, 5, 10, 20], # Minimum number of samples required to split an internal node\n",
        "    'min_samples_leaf': [1, 2, 5, 10], # Minimum number of samples required to be at a leaf node\n",
        "    'max_features': ['sqrt', 'log2', None], # Number of features to consider when looking for the best split\n",
        "    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0], # Fraction of samples used for fitting the individual base learners\n",
        "    # 'criterion': ['friedman_mse', 'squared_error'], # Function to measure the quality of a split (often left default)\n",
        "    # 'loss': ['log_loss', 'deviance', 'exponential'], # Loss function to be optimized\n",
        "}\n",
        "\n",
        "# Initialize the Gradient Boosting Classifier model\n",
        "gb_clf = GradientBoostingClassifier(random_state=42)\n",
        "random_search_gb = RandomizedSearchCV(estimator=gb_clf, param_distributions=param_dist,\n",
        "                                      n_iter=30, cv=5, scoring='accuracy',\n",
        "                                      n_jobs=-1, random_state=42, verbose=2) # verbose=2 shows progress\n",
        "\n",
        "print(\"Starting Randomized Search for Hyperparameter Tuning (Gradient Boosting)...\")\n",
        "\n",
        "# Fit Randomized Search to the training data\n",
        "random_search_gb.fit(X_train, y_train)\n",
        "\n",
        "print(\"Randomized Search finished.\")\n",
        "\n",
        "# Get the best parameters and best score found by Randomized Search\n",
        "best_params_gb = random_search_gb.best_params_\n",
        "best_score_gb = random_search_gb.best_score_\n",
        "\n",
        "print(f\"\\nBest Parameters found by Randomized Search (Gradient Boosting): {best_params_gb}\")\n",
        "print(f\"Best Cross-Validation Score (Accuracy): {best_score_gb:.4f}\")\n",
        "\n",
        "# --- Fit the Algorithm with Best Parameters ---\n",
        "\n",
        "# Initialize the Gradient Boosting Classifier model with the best parameters\n",
        "best_gb_clf = GradientBoostingClassifier(**best_params_gb, random_state=42)\n",
        "\n",
        "print(\"\\nTraining the Gradient Boosting model with best parameters...\")\n",
        "\n",
        "# Fit the model on the entire training data\n",
        "best_gb_clf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Model training completed.\")\n",
        "\n",
        "# --- Predict on the model ---\n",
        "\n",
        "# Predict on the test data using the best model\n",
        "print(\"\\nMaking predictions on the test data using the best Gradient Boosting model...\")\n",
        "y_pred_gb = best_gb_clf.predict(X_test)\n",
        "print(\"Predictions made.\")\n",
        "\n",
        "# --- Model Evaluation (using the best model) ---\n",
        "\n",
        "print(\"\\nGradient Boosting Model Evaluation with Best Parameters:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_gb))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_gb))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_gb))\n",
        "\n",
        "# You can add the metrics from this model to your metrics_data DataFrame for visualization\n",
        "# in the 'Visualizing evaluation Metric Score chart' section."
      ],
      "metadata": {
        "id": "Dn0EOfS6psJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "HAih1iBOpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used RandomizedSearchCV for Gradient Boosting to efficiently explore a wide hyperparameter space with fewer iterations. It reduces computation time while still finding near-optimal parameters."
      ],
      "metadata": {
        "id": "9kBgjYcdpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "zVGeBEFhpsJ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, tuning improved the cross-validation accuracy to 95% and test accuracy to 93.33%. The model now predicts most classes correctly with high precision and recall."
      ],
      "metadata": {
        "id": "74yRdG6UpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. Explain each evaluation metric's indication towards business and the business impact pf the ML model used."
      ],
      "metadata": {
        "id": "bmKjuQ-FpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy ensures overall model correctness, reducing wrong decisions.\n",
        "\n",
        "Precision avoids false positives—crucial for cost-saving and customer trust.\n",
        "\n",
        "Recall ensures we don’t miss important cases—vital in risk or fraud detection.\n",
        "\n",
        "F1-Score balances precision and recall—ensuring reliable, consistent outcomes.\n",
        "Overall, the Gradient Boosting model helps improve decision accuracy, reducing operational errors and enhancing business confidence."
      ],
      "metadata": {
        "id": "BDKtOrBQpsJ3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ML Model - 3"
      ],
      "metadata": {
        "id": "Fze-IPXLpx6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries for RandomForestClassifier and Hyperparameter Tuning\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assuming X_train, X_test, y_train, y_test are already defined from previous code blocks\n",
        "# Ensure that X_train and X_test are properly prepared (one-hot encoded and scaled)\n",
        "# If you are running this cell independently, you need to re-run the data preparation steps from the previous cell.\n",
        "\n",
        "print(\"Starting Random Forest Classifier Model Implementation...\")\n",
        "\n",
        "# --- Hyperparameter Tuning using Randomized Search CV ---\n",
        "# Define the parameter distribution for Random Forest Classifier\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 200, 300, 400, 500, 600, 700, 800, 900, 1000], # Number of trees in the forest\n",
        "    'criterion': ['gini', 'entropy'], # Split quality measure\n",
        "    'max_depth': [None, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100], # Maximum depth of the tree\n",
        "    'min_samples_split': [2, 5, 10, 20, 50], # Minimum number of samples required to split an internal node\n",
        "    'min_samples_leaf': [1, 2, 5, 10, 20], # Minimum number of samples required to be at a leaf node\n",
        "    'max_features': ['sqrt', 'log2', None], # Number of features to consider when looking for the best split\n",
        "    'bootstrap': [True, False], # Whether bootstrap samples are used when building trees\n",
        "    # 'class_weight': [None, 'balanced', 'balanced_subsample'] # Weights associated with classes\n",
        "}\n",
        "\n",
        "# Initialize the Random Forest Classifier model\n",
        "rf_clf = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# Initialize Randomized Search with cross-validation\n",
        "random_search_rf = RandomizedSearchCV(estimator=rf_clf, param_distributions=param_dist,\n",
        "                                      n_iter=50, cv=5, scoring='accuracy',\n",
        "                                      n_jobs=-1, random_state=42, verbose=2) # verbose=2 shows progress\n",
        "\n",
        "print(\"Starting Randomized Search for Hyperparameter Tuning (Random Forest)...\")\n",
        "\n",
        "# Fit Randomized Search to the training data\n",
        "random_search_rf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Randomized Search finished.\")\n",
        "\n",
        "# Get the best parameters and best score found by Randomized Search\n",
        "best_params_rf = random_search_rf.best_params_\n",
        "best_score_rf = random_search_rf.best_score_\n",
        "\n",
        "print(f\"\\nBest Parameters found by Randomized Search (Random Forest): {best_params_rf}\")\n",
        "print(f\"Best Cross-Validation Score (Accuracy): {best_score_rf:.4f}\")\n",
        "\n",
        "# --- Fit the Algorithm with Best Parameters ---\n",
        "\n",
        "# Initialize the Random Forest Classifier model with the best parameters\n",
        "best_rf_clf = RandomForestClassifier(**best_params_rf, random_state=42)\n",
        "\n",
        "print(\"\\nTraining the Random Forest model with best parameters...\")\n",
        "\n",
        "# Fit the model on the entire training data\n",
        "best_rf_clf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Model training completed.\")\n",
        "\n",
        "# --- Predict on the model ---\n",
        "\n",
        "# Predict on the test data using the best model\n",
        "print(\"\\nMaking predictions on the test data using the best Random Forest model...\")\n",
        "y_pred_rf = best_rf_clf.predict(X_test)\n",
        "print(\"Predictions made.\")\n",
        "\n",
        "# --- Model Evaluation (using the best model) ---\n",
        "\n",
        "print(\"\\nRandom Forest Model Evaluation with Best Parameters:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_rf))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_rf))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf))\n",
        "\n",
        "# Remember to add the metrics from this model to your metrics_data DataFrame for visualization."
      ],
      "metadata": {
        "id": "FFrSXAtrpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. Explain the ML Model used and it's performance using Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "7AN1z2sKpx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries for visualization and data manipulation\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd # Ensure pandas is imported to create DataFrames\n",
        "from sklearn.metrics import classification_report # Import classification_report\n",
        "\n",
        "try:\n",
        "    y_pred_lr = best_log_reg.predict(X_test)\n",
        "except NameError:\n",
        "    print(\"best_log_reg model not found. Please run the Logistic Regression cell.\")\n",
        "    y_pred_lr = None # Set to None if model doesn't exist\n",
        "\n",
        "# Get metrics for each model using classification_report and accuracy_score\n",
        "\n",
        "metrics_list = []\n",
        "\n",
        "# Logistic Regression Metrics\n",
        "if y_pred_lr is not None:\n",
        "    lr_report = classification_report(y_test, y_pred_lr, output_dict=True)\n",
        "    metrics_list.append({\n",
        "        'Model': 'Logistic Regression',\n",
        "        'Accuracy': lr_report['accuracy'],\n",
        "        'Precision (Weighted Avg)': lr_report['weighted avg']['precision'],\n",
        "        'Recall (Weighted Avg)': lr_report['weighted avg']['recall'],\n",
        "        'F1-Score (Weighted Avg)': lr_report['weighted avg']['f1-score']\n",
        "    })\n",
        "    print(\"Collected metrics for Logistic Regression.\")\n",
        "\n",
        "# Decision Tree Metrics (using y_pred from the last Decision Tree cell)\n",
        "# Assuming y_pred exists\n",
        "try:\n",
        "    dt_report = classification_report(y_test, y_pred, output_dict=True)\n",
        "    metrics_list.append({\n",
        "        'Model': 'Decision Tree',\n",
        "        'Accuracy': dt_report['accuracy'],\n",
        "        'Precision (Weighted Avg)': dt_report['weighted avg']['precision'],\n",
        "        'Recall (Weighted Avg)': dt_report['weighted avg']['recall'],\n",
        "        'F1-Score (Weighted Avg)': dt_report['weighted avg']['f1-score']\n",
        "    })\n",
        "    print(\"Collected metrics for Decision Tree.\")\n",
        "except NameError:\n",
        "     print(\"y_pred (Decision Tree predictions) not found. Please run the Decision Tree cell.\")\n",
        "\n",
        "\n",
        "# Gradient Boosting Metrics (using y_pred_gb from the Gradient Boosting cell)\n",
        "# Assuming y_pred_gb exists\n",
        "try:\n",
        "    gb_report = classification_report(y_test, y_pred_gb, output_dict=True)\n",
        "    metrics_list.append({\n",
        "        'Model': 'Gradient Boosting',\n",
        "        'Accuracy': gb_report['accuracy'],\n",
        "        'Precision (Weighted Avg)': gb_report['weighted avg']['precision'],\n",
        "        'Recall (Weighted Avg)': gb_report['weighted avg']['recall'],\n",
        "        'F1-Score (Weighted Avg)': gb_report['weighted avg']['f1-score']\n",
        "    })\n",
        "    print(\"Collected metrics for Gradient Boosting.\")\n",
        "except NameError:\n",
        "     print(\"y_pred_gb (Gradient Boosting predictions) not found. Please run the Gradient Boosting cell.\")\n",
        "\n",
        "\n",
        "# Random Forest Metrics (using y_pred_rf from the Random Forest cell)\n",
        "# Assuming y_pred_rf exists\n",
        "try:\n",
        "    rf_report = classification_report(y_test, y_pred_rf, output_dict=True)\n",
        "    metrics_list.append({\n",
        "        'Model': 'Random Forest',\n",
        "        'Accuracy': rf_report['accuracy'],\n",
        "        'Precision (Weighted Avg)': rf_report['weighted avg']['precision'],\n",
        "        'Recall (Weighted Avg)': rf_report['weighted avg']['recall'],\n",
        "        'F1-Score (Weighted Avg)': rf_report['weighted avg']['f1-score']\n",
        "    })\n",
        "    print(\"Collected metrics for Random Forest.\")\n",
        "except NameError:\n",
        "    print(\"y_pred_rf (Random Forest predictions) not found. Please run the Random Forest cell.\")\n",
        "\n",
        "\n",
        "# Create a DataFrame from the list of metrics\n",
        "if metrics_list:\n",
        "    metrics_df = pd.DataFrame(metrics_list)\n",
        "\n",
        "    print(\"\\nEvaluation Metrics Summary:\")\n",
        "    print(metrics_df)\n",
        "\n",
        "    # --- Visualization ---\n",
        "\n",
        "    # Melt the DataFrame to a long format for easier plotting with seaborn\n",
        "    metrics_melted = metrics_df.melt(id_vars='Model', var_name='Metric', value_name='Score')\n",
        "\n",
        "    # Create a bar plot\n",
        "    plt.figure(figsize=(14, 8))\n",
        "    sns.barplot(x='Metric', y='Score', hue='Model', data=metrics_melted, palette='viridis')\n",
        "\n",
        "    # Add titles and labels\n",
        "    plt.title('Comparison of Model Evaluation Metrics', fontsize=18)\n",
        "    plt.xlabel('Evaluation Metric', fontsize=14)\n",
        "    plt.ylabel('Score', fontsize=14)\n",
        "    plt.ylim(0, 1.1) # Set y-limit slightly above 1 for label spacing\n",
        "\n",
        "    # Add value labels on top of bars\n",
        "    ax = plt.gca()\n",
        "    for container in ax.containers:\n",
        "        ax.bar_label(container, fmt='%.3f', label_type='edge', padding=3) # Adjust padding for better spacing\n",
        "\n",
        "    plt.legend(title='Model', bbox_to_anchor=(1.05, 1), loc='upper left') # Place legend outside the plot\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout(rect=[0, 0, 0.85, 1]) # Adjust layout to make space for the legend\n",
        "    plt.show()\n",
        "\n",
        "else:\n",
        "    print(\"\\nNo model metrics were collected. Please ensure previous model training cells were run successfully.\")"
      ],
      "metadata": {
        "id": "xIY4lxxGpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Cross- Validation & Hyperparameter Tuning"
      ],
      "metadata": {
        "id": "9PIHJqyupx6M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries for XGBoost and Hyperparameter Tuning\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from xgboost import XGBClassifier # Import XGBoost Classifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "print(\"Starting XGBoost Classifier Model Implementation...\")\n",
        "y_train_encoded = y_train\n",
        "y_test_encoded = y_test\n",
        "print(\"Target variable is already numerical, no encoding needed.\")\n",
        "\n",
        "\n",
        "# --- Hyperparameter Tuning using Randomized Search CV ---\n",
        "# Define the parameter distribution for XGBoost Classifier\n",
        "param_dist = {\n",
        "    'n_estimators': [100, 200, 300, 400, 500], # Number of boosting rounds\n",
        "    'learning_rate': [0.001, 0.01, 0.05, 0.1, 0.2], # Step size shrinkage\n",
        "    'max_depth': [3, 4, 5, 6, 7, 8], # Maximum depth of a tree\n",
        "    'subsample': [0.6, 0.7, 0.8, 0.9, 1.0], # Fraction of samples used for fitting the individual base learners\n",
        "    'colsample_bytree': [0.6, 0.7, 0.8, 0.9, 1.0], # Fraction of features used per tree\n",
        "    'gamma': [0, 0.1, 0.2, 0.3, 0.4], # Minimum loss reduction required to make a further partition on a leaf node\n",
        "    'lambda': [1, 1.5, 2], # L2 regularization term\n",
        "    'alpha': [0, 0.1, 0.5] # L1 regularization term\n",
        "}\n",
        "\n",
        "# Initialize the XGBoost Classifier model\n",
        "# use_label_encoder=False suppresses a warning, but encoding target is necessary - Not needed if target is already numerical\n",
        "xgb_clf = XGBClassifier(objective='multi:softmax', random_state=42, use_label_encoder=False, eval_metric='mlogloss')\n",
        "# Initialize Randomized Search with cross-validation\n",
        "random_search_xgb = RandomizedSearchCV(estimator=xgb_clf, param_distributions=param_dist,\n",
        "                                       n_iter=30, cv=5, scoring='accuracy', # You can change scoring to 'f1_weighted', etc.\n",
        "                                       n_jobs=-1, random_state=42, verbose=2) # verbose=2 shows progress\n",
        "\n",
        "print(\"Starting Randomized Search for Hyperparameter Tuning (XGBoost)...\")\n",
        "\n",
        "# Fit Randomized Search to the numerical training data\n",
        "random_search_xgb.fit(X_train, y_train) # Use original numerical y_train\n",
        "\n",
        "print(\"Randomized Search finished.\")\n",
        "\n",
        "# Get the best parameters and best score found by Randomized Search\n",
        "best_params_xgb = random_search_xgb.best_params_\n",
        "best_score_xgb = random_search_xgb.best_score_\n",
        "\n",
        "print(f\"\\nBest Parameters found by Randomized Search (XGBoost): {best_params_xgb}\")\n",
        "print(f\"Best Cross-Validation Score (Accuracy): {best_score_xgb:.4f}\")\n",
        "\n",
        "# --- Fit the Algorithm with Best Parameters ---\n",
        "\n",
        "# Initialize the XGBoost Classifier model with the best parameters\n",
        "# use_label_encoder=False and eval_metric are needed here too\n",
        "best_xgb_clf = XGBClassifier(**best_params_xgb, random_state=42, use_label_encoder=False, eval_metric='mlogloss')\n",
        "\n",
        "print(\"\\nTraining the XGBoost model with best parameters...\")\n",
        "\n",
        "# Fit the model on the entire numerical training data\n",
        "best_xgb_clf.fit(X_train, y_train) # Use original numerical y_train\n",
        "\n",
        "print(\"Model training completed.\")\n",
        "\n",
        "# --- Predict on the model ---\n",
        "\n",
        "# Predict on the test data using the best model\n",
        "print(\"\\nMaking predictions on the test data using the best XGBoost model...\")\n",
        "y_pred_xgb = best_xgb_clf.predict(X_test) # Predictions will be numerical\n",
        "print(\"Predictions made.\")\n",
        "\n",
        "print(\"\\nXGBoost Model Evaluation with Best Parameters:\")\n",
        "# Evaluate using original numerical labels and predicted numerical labels\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred_xgb))\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_xgb))\n",
        "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_xgb))"
      ],
      "metadata": {
        "id": "eSVXuaSKpx6M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Which hyperparameter optimization technique have you used and why?"
      ],
      "metadata": {
        "id": "_-qAgymDpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used RandomizedSearchCV for tuning XGBoost because it efficiently searches a wide range of parameters with less computation time than GridSearchCV, making it ideal for models with many hyperparameters."
      ],
      "metadata": {
        "id": "lQMffxkwpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Have you seen any improvement? Note down the improvement with updates Evaluation metric Score Chart."
      ],
      "metadata": {
        "id": "Z-hykwinpx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, after tuning, the accuracy improved from ~93% to 100%. All evaluation metrics (precision, recall, F1-score) reached 1.00, and the confusion matrix showed no misclassifications, confirming the performance gain."
      ],
      "metadata": {
        "id": "MzVzZC6opx6N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Which Evaluation metrics did you consider for a positive business impact and why?"
      ],
      "metadata": {
        "id": "h_CCil-SKHpo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We considered Accuracy, Precision, Recall, and F1-Score. These metrics ensure not just overall correctness but also minimize false positives and false negatives, which is critical for reliable business decisions and customer trust."
      ],
      "metadata": {
        "id": "jHVz9hHDKFms"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Which ML model did you choose from the above created models as your final prediction model and why?"
      ],
      "metadata": {
        "id": "cBFFvTBNJzUa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We selected the XGBoost Classifier as the final model because it achieved 100% accuracy after hyperparameter tuning and outperformed all other models in terms of stability and predictive power."
      ],
      "metadata": {
        "id": "6ksF5Q1LKTVm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Explain the model which you have used and the feature importance using any model explainability tool?"
      ],
      "metadata": {
        "id": "HvGl1hHyA_VK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We used XGBoost, a powerful gradient boosting algorithm. Using XGBoost's feature_importances_ and tools like SHAP (SHapley Additive exPlanations), we identified which features contributed most to predictions, helping improve transparency and trust in model decisions."
      ],
      "metadata": {
        "id": "YnvVTiIxBL-C"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ***8.*** ***Future Work (Optional)***"
      ],
      "metadata": {
        "id": "EyNgTHvd2WFk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Save the best performing ml model in a pickle file or joblib file format for deployment process.\n"
      ],
      "metadata": {
        "id": "KH5McJBi2d8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Imports\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy.stats import uniform\n",
        "import joblib\n",
        "\n",
        "# Optional: Install scikit-optimize if not present\n",
        "try:\n",
        "    from skopt import BayesSearchCV\n",
        "except ImportError:\n",
        "    import os\n",
        "    os.system('pip install scikit-optimize')\n",
        "    from skopt import BayesSearchCV\n",
        "\n",
        "# Load dataset\n",
        "X, y = load_iris(return_X_y=True)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# ---- GridSearchCV ----\n",
        "grid_params = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'solver': ['liblinear', 'lbfgs'],\n",
        "    'penalty': ['l2']\n",
        "}\n",
        "grid_search = GridSearchCV(LogisticRegression(max_iter=200), grid_params, cv=3)\n",
        "grid_search.fit(X_train, y_train)\n",
        "print(\"GridSearch Accuracy:\", accuracy_score(y_test, grid_search.predict(X_test)))\n",
        "joblib.dump(grid_search, 'gridsearch_model.pkl')\n",
        "\n",
        "# ---- RandomizedSearchCV ----\n",
        "random_params = {\n",
        "    'C': uniform(0.01, 10),\n",
        "    'solver': ['liblinear', 'lbfgs'],\n",
        "    'penalty': ['l2']\n",
        "}\n",
        "random_search = RandomizedSearchCV(LogisticRegression(max_iter=200), random_params, n_iter=10, cv=3, random_state=42)\n",
        "random_search.fit(X_train, y_train)\n",
        "print(\"RandomSearch Accuracy:\", accuracy_score(y_test, random_search.predict(X_test)))\n",
        "joblib.dump(random_search, 'randomsearch_model.pkl')\n",
        "\n",
        "# ---- Bayesian Optimization ----\n",
        "bayes_params = {\n",
        "    'C': (1e-3, 1e+2, 'log-uniform'),\n",
        "    'solver': ['liblinear', 'lbfgs']\n",
        "}\n",
        "bayes_search = BayesSearchCV(LogisticRegression(max_iter=200), search_spaces=bayes_params, n_iter=10, cv=3, random_state=42)\n",
        "bayes_search.fit(X_train, y_train)\n",
        "print(\"Bayesian Optimization Accuracy:\", accuracy_score(y_test, bayes_search.predict(X_test)))\n",
        "joblib.dump(bayes_search, 'bayesopt_model.pkl')\n"
      ],
      "metadata": {
        "id": "bQIANRl32f4J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Again Load the saved model file and try to predict unseen data for a sanity check.\n"
      ],
      "metadata": {
        "id": "iW_Lq9qf2h6X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the File and predict unseen data.\n",
        "try:\n",
        "    # Load the Bayesian Optimization model saved in the previous cell\n",
        "    loaded_model = joblib.load('bayesopt_model.pkl')\n",
        "    print(\"Model loaded successfully!\")\n",
        "\n",
        "    unseen_data = np.array([[5.0, 3.5, 1.3, 0.2],  # Example data point 1\n",
        "                            [6.5, 3.0, 5.2, 2.0]]) # Example data point 2\n",
        "\n",
        "    # Ensure the unseen data has the correct shape (n_samples, n_features)\n",
        "    # X_train was defined in the previous cell (ipython-input-21)\n",
        "    if unseen_data.shape[1] != X_train.shape[1]:\n",
        "        print(f\"Error: Unseen data has {unseen_data.shape[1]} features, but the model was trained on data with {X_train.shape[1]} features.\")\n",
        "        print(\"Please provide unseen data with the correct number of features.\")\n",
        "    else:\n",
        "        # Predict using the loaded model\n",
        "        unseen_predictions = loaded_model.predict(unseen_data)\n",
        "\n",
        "        print(\"\\nPredictions for unseen data:\")\n",
        "        print(unseen_predictions)\n",
        "\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: Model file not found. Please ensure 'bayesopt_model.pkl' exists.\")\n",
        "    print(\"Run the previous cell to train and save the model before running this cell.\")\n",
        "\n",
        "# Note: X_train must be defined in a preceding cell for the shape check to work.\n",
        "# Ensure the cell where X_train is defined (ipython-input-21 in this case) is run first."
      ],
      "metadata": {
        "id": "oEXk9ydD2nVC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Congrats! Your model is successfully created and ready for deployment on a live server for a real user interaction !!!***"
      ],
      "metadata": {
        "id": "-Kee-DAl2viO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Conclusion**"
      ],
      "metadata": {
        "id": "gCX9965dhzqZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project, we implemented Logistic Regression on the Iris dataset using three hyperparameter optimization techniques: GridSearchCV, RandomizedSearchCV, and Bayesian Optimization. Each method successfully identified the best combination of parameters, improving model performance. GridSearch performed exhaustive search, RandomizedSearch provided faster convergence with random sampling, and Bayesian Optimization efficiently explored the parameter space using prior results. All models achieved high accuracy on the test data. The trained models were saved for future use. This comparison highlights the trade-off between search time and optimization effectiveness, guiding practitioners to choose the best tuning strategy based on data size and complexity."
      ],
      "metadata": {
        "id": "Fjb1IsQkh3yE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ***Hurrah! You have successfully completed your Machine Learning Capstone Project !!!***"
      ],
      "metadata": {
        "id": "gIfDvo9L0UH2"
      }
    }
  ]
}